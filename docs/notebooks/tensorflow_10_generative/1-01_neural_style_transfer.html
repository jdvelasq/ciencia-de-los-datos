<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2018 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Copyright 2018 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/tensorflow_10_generative/1-01_neural_style_transfer.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2018-The-TensorFlow-Authors.">
<h1>Copyright 2018 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2018-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Neural-style-transfer">
<h2>Neural style transfer<a class="headerlink" href="#Neural-style-transfer" title="Enlazar permanentemente con este título"></a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>37d5f2ff2a404f7ba04af33b7c99480c|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>4d22a986d04e4017aee8f7695b91f4f0|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>c22998d3b2084ef1925fba41d7618a77|View on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>ff09e36ae9df497ab9d848b3f52a89a0|Download notebook</p>
</td><td><p><a href="#id9"><span class="problematic" id="id10">|</span></a>0e8dd3c2298542b398608bcffd9308ad|See TF Hub model</p>
</td></table><p>This tutorial uses deep learning to compose one image in the style of another image (ever wish you could paint like Picasso or Van Gogh?). This is known as <em>neural style transfer</em> and the technique is outlined in A Neural Algorithm of Artistic Style (Gatys et al.).</p>
<p>Note: This tutorial demonstrates the original style-transfer algorithm. It optimizes the image content to a particular style. Modern approaches train a model to generate the stylized image directly (similar to <a class="reference external" href="cyclegan.ipynb">cyclegan</a>). This approach is much faster (up to 1000x).</p>
<p>For a simple application of style transfer check out this <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization">tutorial</a> to learn more about how to use the pretrained <a class="reference external" href="https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2">Arbitrary Image Stylization model</a> from <a class="reference external" href="https://tfhub.dev">TensorFlow Hub</a> or how to use a style transfer model with <a class="reference external" href="https://www.tensorflow.org/lite/models/style_transfer/overview">TensorFlow Lite</a>.</p>
<p>Neural style transfer is an optimization technique used to take two images—a <em>content</em> image and a <em>style reference</em> image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.</p>
<p>This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.</p>
<p>For example, let’s take an image of this dog and Wassily Kandinsky’s Composition 7:</p>
<p><img alt="295f41a333f341a888fa559a3aebe73e" class="no-scaled-link" src="https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg" style="width: 500px;" /></p>
<p><a class="reference external" href="https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg">Yellow Labrador Looking</a>, from Wikimedia Commons by <a class="reference external" href="https://en.wikipedia.org/wiki/User:Elf">Elf</a>. License <a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">CC BY-SA 3.0</a></p>
<p><img alt="e1798e0dc0b84ae4a685bad31eb8c896" class="no-scaled-link" src="https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg" style="width: 500px;" /></p>
<p>Now how would it look like if Kandinsky decided to paint the picture of this Dog exclusively with this style? Something like this?</p>
<p><img alt="8dc2030efe6248909985c748820f633f" src="https://tensorflow.org/tutorials/generative/images/stylized-image.png" /></p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título"></a></h3>
<div class="section" id="Import-and-configure-modules">
<h4>Import and configure modules<a class="headerlink" href="#Import-and-configure-modules" title="Enlazar permanentemente con este título"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import os
import tensorflow as tf
# Load compressed models from tensorflow_hub
os.environ[&#39;TFHUB_MODEL_LOAD_FORMAT&#39;] = &#39;COMPRESSED&#39;
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams[&#39;figure.figsize&#39;] = (12, 12)
mpl.rcParams[&#39;axes.grid&#39;] = False

import numpy as np
import PIL.Image
import time
import functools
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)&gt;3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)
</pre></div>
</div>
</div>
<p>Download images and choose a style image and a content image:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>content_path = tf.keras.utils.get_file(&#39;YellowLabradorLooking_new.jpg&#39;, &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg&#39;)
style_path = tf.keras.utils.get_file(&#39;kandinsky5.jpg&#39;,&#39;https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg&#39;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Visualize-the-input">
<h3>Visualize the input<a class="headerlink" href="#Visualize-the-input" title="Enlazar permanentemente con este título"></a></h3>
<p>Define a function to load an image and limit its maximum dimension to 512 pixels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img
</pre></div>
</div>
</div>
<p>Create a simple function to display an image:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def imshow(image, title=None):
  if len(image.shape) &gt; 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>content_image = load_img(content_path)
style_image = load_img(style_path)

plt.subplot(1, 2, 1)
imshow(content_image, &#39;Content Image&#39;)

plt.subplot(1, 2, 2)
imshow(style_image, &#39;Style Image&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Fast-Style-Transfer-using-TF-Hub">
<h3>Fast Style Transfer using TF-Hub<a class="headerlink" href="#Fast-Style-Transfer-using-TF-Hub" title="Enlazar permanentemente con este título"></a></h3>
<p>This tutorial demonstrates the original style-transfer algorithm, which optimizes the image content to a particular style. Before getting into the details, let’s see how the <a class="reference external" href="https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2">TensorFlow Hub model</a> does this:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow_hub as hub
hub_model = hub.load(&#39;https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2&#39;)
stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]
tensor_to_image(stylized_image)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-content-and-style-representations">
<h3>Define content and style representations<a class="headerlink" href="#Define-content-and-style-representations" title="Enlazar permanentemente con este título"></a></h3>
<p>Use the intermediate layers of the model to get the <em>content</em> and <em>style</em> representations of the image. Starting from the network’s input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level features—object parts like <em>wheels</em> or <em>eyes</em>. In this case, you are using the VGG19 network architecture, a pretrained image classification network. These intermediate layers are necessary to
define the representation of content and style from the images. For an input image, try to match the corresponding style and content target representations at these intermediate layers.</p>
<p>Load a <a class="reference external" href="https://keras.io/api/applications/vgg/#vgg19-function">VGG19</a> and test run it on our image to ensure it’s used correctly:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x = tf.keras.applications.vgg19.preprocess_input(content_image*255)
x = tf.image.resize(x, (224, 224))
vgg = tf.keras.applications.VGG19(include_top=True, weights=&#39;imagenet&#39;)
prediction_probabilities = vgg(x)
prediction_probabilities.shape
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]
[(class_name, prob) for (number, class_name, prob) in predicted_top_5]
</pre></div>
</div>
</div>
<p>Now load a <code class="docutils literal notranslate"><span class="pre">VGG19</span></code> without the classification head, and list the layer names</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>vgg = tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;)

print()
for layer in vgg.layers:
  print(layer.name)
</pre></div>
</div>
</div>
<p>Choose intermediate layers from the network to represent the style and content of the image:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>content_layers = [&#39;block5_conv2&#39;]

style_layers = [&#39;block1_conv1&#39;,
                &#39;block2_conv1&#39;,
                &#39;block3_conv1&#39;,
                &#39;block4_conv1&#39;,
                &#39;block5_conv1&#39;]

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)
</pre></div>
</div>
</div>
<p>So why do these intermediate outputs within our pretrained image classification network allow us to define style and content representations?</p>
<p>At a high level, in order for a network to perform image classification (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image.</p>
<p>This is also a reason why convolutional neural networks are able to generalize well: they’re able to capture the invariances and defining features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you’re able to describe the content and style of input
images.</p>
</div>
<div class="section" id="Build-the-model">
<h3>Build the model<a class="headerlink" href="#Build-the-model" title="Enlazar permanentemente con este título"></a></h3>
<p>The networks in <code class="docutils literal notranslate"><span class="pre">tf.keras.applications</span></code> are designed so you can easily extract the intermediate layer values using the Keras functional API.</p>
<p>To define a model using the functional API, specify the inputs and outputs:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Model(inputs,</span> <span class="pre">outputs)</span></code></p>
<p>This following function builds a VGG19 model that returns a list of intermediate layer outputs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def vgg_layers(layer_names):
  &quot;&quot;&quot; Creates a vgg model that returns a list of intermediate output values.&quot;&quot;&quot;
  # Load our model. Load pretrained VGG, trained on imagenet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;)
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model
</pre></div>
</div>
</div>
<p>And to create the model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>style_extractor = vgg_layers(style_layers)
style_outputs = style_extractor(style_image*255)

#Look at the statistics of each layer&#39;s output
for name, output in zip(style_layers, style_outputs):
  print(name)
  print(&quot;  shape: &quot;, output.numpy().shape)
  print(&quot;  min: &quot;, output.numpy().min())
  print(&quot;  max: &quot;, output.numpy().max())
  print(&quot;  mean: &quot;, output.numpy().mean())
  print()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Calculate-style">
<h3>Calculate style<a class="headerlink" href="#Calculate-style" title="Enlazar permanentemente con este título"></a></h3>
<p>The content of an image is represented by the values of the intermediate feature maps.</p>
<p>It turns out, the style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. This Gram matrix can be calculated for a particular layer as:</p>
<div class="math notranslate nohighlight">
\[G^l_{cd} = \frac{\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}\]</div>
<p>This can be implemented concisely using the <code class="docutils literal notranslate"><span class="pre">tf.linalg.einsum</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def gram_matrix(input_tensor):
  result = tf.linalg.einsum(&#39;bijc,bijd-&gt;bcd&#39;, input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  return result/(num_locations)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Extract-style-and-content">
<h3>Extract style and content<a class="headerlink" href="#Extract-style-and-content" title="Enlazar permanentemente con este título"></a></h3>
<p>Build a model that returns the style and content tensors.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg = vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    &quot;Expects float input in [0,1]&quot;
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name: value
                    for content_name, value
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name: value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}

    return {&#39;content&#39;: content_dict, &#39;style&#39;: style_dict}
</pre></div>
</div>
</div>
<p>When called on an image, this model returns the gram matrix (style) of the <code class="docutils literal notranslate"><span class="pre">style_layers</span></code> and content of the <code class="docutils literal notranslate"><span class="pre">content_layers</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>extractor = StyleContentModel(style_layers, content_layers)

results = extractor(tf.constant(content_image))

print(&#39;Styles:&#39;)
for name, output in sorted(results[&#39;style&#39;].items()):
  print(&quot;  &quot;, name)
  print(&quot;    shape: &quot;, output.numpy().shape)
  print(&quot;    min: &quot;, output.numpy().min())
  print(&quot;    max: &quot;, output.numpy().max())
  print(&quot;    mean: &quot;, output.numpy().mean())
  print()

print(&quot;Contents:&quot;)
for name, output in sorted(results[&#39;content&#39;].items()):
  print(&quot;  &quot;, name)
  print(&quot;    shape: &quot;, output.numpy().shape)
  print(&quot;    min: &quot;, output.numpy().min())
  print(&quot;    max: &quot;, output.numpy().max())
  print(&quot;    mean: &quot;, output.numpy().mean())

</pre></div>
</div>
</div>
</div>
<div class="section" id="Run-gradient-descent">
<h3>Run gradient descent<a class="headerlink" href="#Run-gradient-descent" title="Enlazar permanentemente con este título"></a></h3>
<p>With this style and content extractor, you can now implement the style transfer algorithm. Do this by calculating the mean square error for your image’s output relative to each target, then take the weighted sum of these losses.</p>
<p>Set your style and content target values:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>style_targets = extractor(style_image)[&#39;style&#39;]
content_targets = extractor(content_image)[&#39;content&#39;]
</pre></div>
</div>
</div>
<p>Define a <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> to contain the image to optimize. To make this quick, initialize it with the content image (the <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code> must be the same shape as the content image):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>image = tf.Variable(content_image)
</pre></div>
</div>
</div>
<p>Since this is a float image, define a function to keep the pixel values between 0 and 1:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
</pre></div>
</div>
</div>
<p>Create an optimizer. The paper recommends LBFGS, but <code class="docutils literal notranslate"><span class="pre">Adam</span></code> works okay, too:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
</pre></div>
</div>
</div>
<p>To optimize this, use a weighted combination of the two losses to get the total loss:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>style_weight=1e-2
content_weight=1e4
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def style_content_loss(outputs):
    style_outputs = outputs[&#39;style&#39;]
    content_outputs = outputs[&#39;content&#39;]
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    return loss
</pre></div>
</div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">tf.GradientTape</span></code> to update the image.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))
</pre></div>
</div>
</div>
<p>Now run a few steps to test:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_step(image)
train_step(image)
train_step(image)
tensor_to_image(image)
</pre></div>
</div>
</div>
<p>Since it’s working, perform a longer optimization:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import time
start = time.time()

epochs = 10
steps_per_epoch = 100

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(&quot;.&quot;, end=&#39;&#39;, flush=True)
  display.clear_output(wait=True)
  display.display(tensor_to_image(image))
  print(&quot;Train step: {}&quot;.format(step))

end = time.time()
print(&quot;Total time: {:.1f}&quot;.format(end-start))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Total-variation-loss">
<h3>Total variation loss<a class="headerlink" href="#Total-variation-loss" title="Enlazar permanentemente con este título"></a></h3>
<p>One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the <em>total variation loss</em>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def high_pass_x_y(image):
  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]
  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]

  return x_var, y_var
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>x_deltas, y_deltas = high_pass_x_y(content_image)

plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
imshow(clip_0_1(2*y_deltas+0.5), &quot;Horizontal Deltas: Original&quot;)

plt.subplot(2, 2, 2)
imshow(clip_0_1(2*x_deltas+0.5), &quot;Vertical Deltas: Original&quot;)

x_deltas, y_deltas = high_pass_x_y(image)

plt.subplot(2, 2, 3)
imshow(clip_0_1(2*y_deltas+0.5), &quot;Horizontal Deltas: Styled&quot;)

plt.subplot(2, 2, 4)
imshow(clip_0_1(2*x_deltas+0.5), &quot;Vertical Deltas: Styled&quot;)
</pre></div>
</div>
</div>
<p>This shows how the high frequency components have increased.</p>
<p>Also, this high frequency component is basically an edge-detector. You can get similar output from the Sobel edge detector, for example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.figure(figsize=(14, 10))

sobel = tf.image.sobel_edges(content_image)
plt.subplot(1, 2, 1)
imshow(clip_0_1(sobel[..., 0]/4+0.5), &quot;Horizontal Sobel-edges&quot;)
plt.subplot(1, 2, 2)
imshow(clip_0_1(sobel[..., 1]/4+0.5), &quot;Vertical Sobel-edges&quot;)
</pre></div>
</div>
</div>
<p>The regularization loss associated with this is the sum of the squares of the values:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def total_variation_loss(image):
  x_deltas, y_deltas = high_pass_x_y(image)
  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>total_variation_loss(image).numpy()
</pre></div>
</div>
</div>
<p>That demonstrated what it does. But there’s no need to implement it yourself, TensorFlow includes a standard implementation:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.image.total_variation(image).numpy()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Re-run-the-optimization">
<h3>Re-run the optimization<a class="headerlink" href="#Re-run-the-optimization" title="Enlazar permanentemente con este título"></a></h3>
<p>Choose a weight for the <code class="docutils literal notranslate"><span class="pre">total_variation_loss</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>total_variation_weight=30
</pre></div>
</div>
</div>
<p>Now include it in the <code class="docutils literal notranslate"><span class="pre">train_step</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)
    loss += total_variation_weight*tf.image.total_variation(image)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))
</pre></div>
</div>
</div>
<p>Reinitialize the optimization variable:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>image = tf.Variable(content_image)
</pre></div>
</div>
</div>
<p>And run the optimization:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import time
start = time.time()

epochs = 10
steps_per_epoch = 100

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(&quot;.&quot;, end=&#39;&#39;, flush=True)
  display.clear_output(wait=True)
  display.display(tensor_to_image(image))
  print(&quot;Train step: {}&quot;.format(step))

end = time.time()
print(&quot;Total time: {:.1f}&quot;.format(end-start))
</pre></div>
</div>
</div>
<p>Finally, save the result:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>file_name = &#39;stylized-image.png&#39;
tensor_to_image(image).save(file_name)

try:
  from google.colab import files
except ImportError:
   pass
else:
  files.download(file_name)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Learn-more">
<h3>Learn more<a class="headerlink" href="#Learn-more" title="Enlazar permanentemente con este título"></a></h3>
<p>This tutorial demonstrates the original style-transfer algorithm. For a simple application of style transfer check out this <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization">tutorial</a> to learn more about how to use the arbitrary image style transfer model from <a class="reference external" href="https://tfhub.dev">TensorFlow Hub</a>.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>