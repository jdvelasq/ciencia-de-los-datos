<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2021 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Copyright 2021 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/deep_learning_09_tf_audio/1-02_transfer_learning_for_audio_recognition.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2021-The-TensorFlow-Authors.">
<h1>Copyright 2021 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2021-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>213ea2619b6b44bcad9c23ab44d652dc|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>abbf5d47d85845e5926562ba6e75a7a6|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>64e94e79f5574f6aaf2c2cfbe424ea92|View on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>0c044610bad64b92bcd4c0325a349492|Download notebook</p>
</td><td><p><a href="#id9"><span class="problematic" id="id10">|</span></a>aae7fb49958f4bd186afff60cfc8991a|See TF Hub model</p>
</td></table><div class="section" id="Transfer-learning-with-YAMNet-for-environmental-sound-classification">
<h2>Transfer learning with YAMNet for environmental sound classification<a class="headerlink" href="#Transfer-learning-with-YAMNet-for-environmental-sound-classification" title="Enlazar permanentemente con este título"></a></h2>
<p><a class="reference external" href="https://tfhub.dev/google/yamnet/1">YAMNet</a> is a pre-trained deep neural network that can predict audio events from <a class="reference external" href="https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv">521 classes</a>, such as laughter, barking, or a siren.</p>
<p>In this tutorial you will learn how to:</p>
<ul class="simple">
<li><p>Load and use the YAMNet model for inference.</p></li>
<li><p>Build a new model using the YAMNet embeddings to classify cat and dog sounds.</p></li>
<li><p>Evaluate and export your model.</p></li>
</ul>
<div class="section" id="Import-TensorFlow-and-other-libraries">
<h3>Import TensorFlow and other libraries<a class="headerlink" href="#Import-TensorFlow-and-other-libraries" title="Enlazar permanentemente con este título"></a></h3>
<p>Start by installing <a class="reference external" href="https://www.tensorflow.org/io">TensorFlow I/O</a>, which will make it easier for you to load audio files off disk.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!pip install tensorflow_io
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import os

from IPython import display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_io as tfio
</pre></div>
</div>
</div>
</div>
<div class="section" id="About-YAMNet">
<h3>About YAMNet<a class="headerlink" href="#About-YAMNet" title="Enlazar permanentemente con este título"></a></h3>
<p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/audioset/yamnet">YAMNet</a> is a pre-trained neural network that employs the <a class="reference external" href="https://arxiv.org/abs/1704.04861">MobileNetV1</a> depthwise-separable convolution architecture. It can use an audio waveform as input and make independent predictions for each of the 521 audio events from the <a class="reference external" href="http://g.co/audioset">AudioSet</a> corpus.</p>
<p>Internally, the model extracts “frames” from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds .</p>
<p>The model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range <code class="docutils literal notranslate"><span class="pre">[-1.0,</span> <span class="pre">+1.0]</span></code>. This tutorial contains code to help you convert WAV files into the supported format.</p>
<p>The model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel <a class="reference external" href="https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram">spectrogram</a>. You can find more details <a class="reference external" href="https://tfhub.dev/google/yamnet/1">here</a>.</p>
<p>One specific use of YAMNet is as a high-level feature extractor - the 1,024-dimensional embedding output. You will use the base (YAMNet) model’s input features and feed them into your shallower model consisting of one hidden <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> layer. Then, you will train the network on a small amount of data for audio classification <em>without</em> requiring a lot of labeled data and training end-to-end. (This is similar to <a class="reference external" href="https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub">transfer learning for image classification with TensorFlow
Hub</a> for more information.)</p>
<p>First, you will test the model and see the results of classifying audio. You will then construct the data pre-processing pipeline.</p>
<div class="section" id="Loading-YAMNet-from-TensorFlow-Hub">
<h4>Loading YAMNet from TensorFlow Hub<a class="headerlink" href="#Loading-YAMNet-from-TensorFlow-Hub" title="Enlazar permanentemente con este título"></a></h4>
<p>You are going to use a pre-trained YAMNet from <a class="reference external" href="https://tfhub.dev/">Tensorflow Hub</a> to extract the embeddings from the sound files.</p>
<p>Loading a model from TensorFlow Hub is straightforward: choose the model, copy its URL, and use the <code class="docutils literal notranslate"><span class="pre">load</span></code> function.</p>
<p>Note: to read the documentation of the model, use the model URL in your browser.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>yamnet_model_handle = &#39;https://tfhub.dev/google/yamnet/1&#39;
yamnet_model = hub.load(yamnet_model_handle)
</pre></div>
</div>
</div>
<p>With the model loaded, you can follow the <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/yamnet">YAMNet basic usage tutorial</a> and download a sample WAV file to run the inference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>testing_wav_file_name = tf.keras.utils.get_file(&#39;miaow_16k.wav&#39;,
                                                &#39;https://storage.googleapis.com/audioset/miaow_16k.wav&#39;,
                                                cache_dir=&#39;./&#39;,
                                                cache_subdir=&#39;test_data&#39;)

print(testing_wav_file_name)
</pre></div>
</div>
</div>
<p>You will need a function to load audio files, which will also be used later when working with the training data. (Learn more about reading audio files and their labels in <a class="reference external" href="https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels">Simple audio recognition</a>.</p>
<p>Note: The returned <code class="docutils literal notranslate"><span class="pre">wav_data</span></code> from <code class="docutils literal notranslate"><span class="pre">load_wav_16k_mono</span></code> is already normalized to values in the <code class="docutils literal notranslate"><span class="pre">[-1.0,</span> <span class="pre">1.0]</span></code> range (for more information, go to <a class="reference external" href="https://tfhub.dev/google/yamnet/1">YAMNet’s documentation on TF Hub</a>).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Utility functions for loading audio files and making sure the sample rate is correct.

@tf.function
def load_wav_16k_mono(filename):
    &quot;&quot;&quot; Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. &quot;&quot;&quot;
    file_contents = tf.io.read_file(filename)
    wav, sample_rate = tf.audio.decode_wav(
          file_contents,
          desired_channels=1)
    wav = tf.squeeze(wav, axis=-1)
    sample_rate = tf.cast(sample_rate, dtype=tf.int64)
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
    return wav
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>testing_wav_data = load_wav_16k_mono(testing_wav_file_name)

_ = plt.plot(testing_wav_data)

# Play the audio file.
display.Audio(testing_wav_data,rate=16000)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-the-class-mapping">
<h4>Load the class mapping<a class="headerlink" href="#Load-the-class-mapping" title="Enlazar permanentemente con este título"></a></h4>
<p>It’s important to load the class names that YAMNet is able to recognize. The mapping file is present at <code class="docutils literal notranslate"><span class="pre">yamnet_model.class_map_path()</span></code> in the CSV format.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class_map_path = yamnet_model.class_map_path().numpy().decode(&#39;utf-8&#39;)
class_names =list(pd.read_csv(class_map_path)[&#39;display_name&#39;])

for name in class_names[:20]:
  print(name)
print(&#39;...&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Run-inference">
<h4>Run inference<a class="headerlink" href="#Run-inference" title="Enlazar permanentemente con este título"></a></h4>
<p>YAMNet provides frame-level class-scores (i.e., 521 scores for every frame). In order to determine clip-level predictions, the scores can be aggregated per-class across frames (e.g., using mean or max aggregation). This is done below by <code class="docutils literal notranslate"><span class="pre">scores_np.mean(axis=0)</span></code>. Finally, to find the top-scored class at the clip-level, you take the maximum of the 521 aggregated scores.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
class_scores = tf.reduce_mean(scores, axis=0)
top_class = tf.argmax(class_scores)
inferred_class = class_names[top_class]

print(f&#39;The main sound is: {inferred_class}&#39;)
print(f&#39;The embeddings shape: {embeddings.shape}&#39;)
</pre></div>
</div>
</div>
<p>Note: The model correctly inferred an animal sound. Your goal in this tutorial is to increase the model’s accuracy for specific classes. Also, notice that the model generated 13 embeddings, 1 per frame.</p>
</div>
</div>
<div class="section" id="ESC-50-dataset">
<h3>ESC-50 dataset<a class="headerlink" href="#ESC-50-dataset" title="Enlazar permanentemente con este título"></a></h3>
<p>The <a class="reference external" href="https://github.com/karolpiczak/ESC-50#repository-content">ESC-50 dataset</a> (<a class="reference external" href="https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf">Piczak, 2015</a>) is a labeled collection of 2,000 five-second long environmental audio recordings. The dataset consists of 50 classes, with 40 examples per class.</p>
<p>Download the dataset and extract it.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>_ = tf.keras.utils.get_file(&#39;esc-50.zip&#39;,
                        &#39;https://github.com/karoldvl/ESC-50/archive/master.zip&#39;,
                        cache_dir=&#39;./&#39;,
                        cache_subdir=&#39;datasets&#39;,
                        extract=True)
</pre></div>
</div>
</div>
<div class="section" id="Explore-the-data">
<h4>Explore the data<a class="headerlink" href="#Explore-the-data" title="Enlazar permanentemente con este título"></a></h4>
<p>The metadata for each file is specified in the csv file at <code class="docutils literal notranslate"><span class="pre">./datasets/ESC-50-master/meta/esc50.csv</span></code></p>
<p>and all the audio files are in <code class="docutils literal notranslate"><span class="pre">./datasets/ESC-50-master/audio/</span></code></p>
<p>You will create a pandas <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> with the mapping and use that to have a clearer view of the data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>esc50_csv = &#39;./datasets/ESC-50-master/meta/esc50.csv&#39;
base_data_path = &#39;./datasets/ESC-50-master/audio/&#39;

pd_data = pd.read_csv(esc50_csv)
pd_data.head()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Filter-the-data">
<h4>Filter the data<a class="headerlink" href="#Filter-the-data" title="Enlazar permanentemente con este título"></a></h4>
<p>Now that the data is stored in the <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, apply some transformations:</p>
<ul class="simple">
<li><p>Filter out rows and use only the selected classes - <code class="docutils literal notranslate"><span class="pre">dog</span></code> and <code class="docutils literal notranslate"><span class="pre">cat</span></code>. If you want to use any other classes, this is where you can choose them.</p></li>
<li><p>Amend the filename to have the full path. This will make loading easier later.</p></li>
<li><p>Change targets to be within a specific range. In this example, <code class="docutils literal notranslate"><span class="pre">dog</span></code> will remain at <code class="docutils literal notranslate"><span class="pre">0</span></code>, but <code class="docutils literal notranslate"><span class="pre">cat</span></code> will become <code class="docutils literal notranslate"><span class="pre">1</span></code> instead of its original value of <code class="docutils literal notranslate"><span class="pre">5</span></code>.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>my_classes = [&#39;dog&#39;, &#39;cat&#39;]
map_class_to_id = {&#39;dog&#39;:0, &#39;cat&#39;:1}

filtered_pd = pd_data[pd_data.category.isin(my_classes)]

class_id = filtered_pd[&#39;category&#39;].apply(lambda name: map_class_to_id[name])
filtered_pd = filtered_pd.assign(target=class_id)

full_path = filtered_pd[&#39;filename&#39;].apply(lambda row: os.path.join(base_data_path, row))
filtered_pd = filtered_pd.assign(filename=full_path)

filtered_pd.head(10)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-the-audio-files-and-retrieve-embeddings">
<h4>Load the audio files and retrieve embeddings<a class="headerlink" href="#Load-the-audio-files-and-retrieve-embeddings" title="Enlazar permanentemente con este título"></a></h4>
<p>Here you’ll apply the <code class="docutils literal notranslate"><span class="pre">load_wav_16k_mono</span></code> and prepare the WAV data for the model.</p>
<p>When extracting embeddings from the WAV data, you get an array of shape <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">1024)</span></code> where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of frames that YAMNet found (one for every 0.48 seconds of audio).</p>
<p>Your model will use each frame as one input. Therefore, you need to create a new column that has one frame per row. You also need to expand the labels and the <code class="docutils literal notranslate"><span class="pre">fold</span></code> column to proper reflect these new rows.</p>
<p>The expanded <code class="docutils literal notranslate"><span class="pre">fold</span></code> column keeps the original values. You cannot mix frames because, when performing the splits, you might end up having parts of the same audio on different splits, which would make your validation and test steps less effective.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>filenames = filtered_pd[&#39;filename&#39;]
targets = filtered_pd[&#39;target&#39;]
folds = filtered_pd[&#39;fold&#39;]

main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))
main_ds.element_spec
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def load_wav_for_map(filename, label, fold):
  return load_wav_16k_mono(filename), label, fold

main_ds = main_ds.map(load_wav_for_map)
main_ds.element_spec
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># applies the embedding extraction model to a wav data
def extract_embedding(wav_data, label, fold):
  &#39;&#39;&#39; run YAMNet to extract embedding from the wav data &#39;&#39;&#39;
  scores, embeddings, spectrogram = yamnet_model(wav_data)
  num_embeddings = tf.shape(embeddings)[0]
  return (embeddings,
            tf.repeat(label, num_embeddings),
            tf.repeat(fold, num_embeddings))

# extract embedding
main_ds = main_ds.map(extract_embedding).unbatch()
main_ds.element_spec
</pre></div>
</div>
</div>
</div>
<div class="section" id="Split-the-data">
<h4>Split the data<a class="headerlink" href="#Split-the-data" title="Enlazar permanentemente con este título"></a></h4>
<p>You will use the <code class="docutils literal notranslate"><span class="pre">fold</span></code> column to split the dataset into train, validation and test sets.</p>
<p>ESC-50 is arranged into five uniformly-sized cross-validation <code class="docutils literal notranslate"><span class="pre">fold</span></code>s, such that clips from the same original source are always in the same <code class="docutils literal notranslate"><span class="pre">fold</span></code> - find out more in the <a class="reference external" href="https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf">ESC: Dataset for Environmental Sound Classification</a> paper.</p>
<p>The last step is to remove the <code class="docutils literal notranslate"><span class="pre">fold</span></code> column from the dataset since you’re not going to use it during training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cached_ds = main_ds.cache()
train_ds = cached_ds.filter(lambda embedding, label, fold: fold &lt; 4)
val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)
test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)

# remove the folds column now that it&#39;s not needed anymore
remove_fold_column = lambda embedding, label, fold: (embedding, label)

train_ds = train_ds.map(remove_fold_column)
val_ds = val_ds.map(remove_fold_column)
test_ds = test_ds.map(remove_fold_column)

train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Create-your-model">
<h3>Create your model<a class="headerlink" href="#Create-your-model" title="Enlazar permanentemente con este título"></a></h3>
<p>You did most of the work! Next, define a very simple <a class="reference external" href="https://www.tensorflow.org/guide/keras/sequential_model">Sequential</a> model with one hidden layer and two outputs to recognize cats and dogs from sounds.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>my_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,
                          name=&#39;input_embedding&#39;),
    tf.keras.layers.Dense(512, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(len(my_classes))
], name=&#39;my_model&#39;)

my_model.summary()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 optimizer=&quot;adam&quot;,
                 metrics=[&#39;accuracy&#39;])

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;,
                                            patience=3,
                                            restore_best_weights=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>history = my_model.fit(train_ds,
                       epochs=20,
                       validation_data=val_ds,
                       callbacks=callback)
</pre></div>
</div>
</div>
<p>Let’s run the <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> method on the test data just to be sure there’s no overfitting.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>loss, accuracy = my_model.evaluate(test_ds)

print(&quot;Loss: &quot;, loss)
print(&quot;Accuracy: &quot;, accuracy)
</pre></div>
</div>
</div>
<p>You did it!</p>
</div>
<div class="section" id="Test-your-model">
<h3>Test your model<a class="headerlink" href="#Test-your-model" title="Enlazar permanentemente con este título"></a></h3>
<p>Next, try your model on the embedding from the previous test using YAMNet only.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
result = my_model(embeddings).numpy()

inferred_class = my_classes[result.mean(axis=0).argmax()]
print(f&#39;The main sound is: {inferred_class}&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Save-a-model-that-can-directly-take-a-WAV-file-as-input">
<h3>Save a model that can directly take a WAV file as input<a class="headerlink" href="#Save-a-model-that-can-directly-take-a-WAV-file-as-input" title="Enlazar permanentemente con este título"></a></h3>
<p>Your model works when you give it the embeddings as input.</p>
<p>In a real-world scenario, you’ll want to use audio data as a direct input.</p>
<p>To do that, you will combine YAMNet with your model into a single model that you can export for other applications.</p>
<p>To make it easier to use the model’s result, the final layer will be a <code class="docutils literal notranslate"><span class="pre">reduce_mean</span></code> operation. When using this model for serving (which you will learn about later in the tutorial), you will need the name of the final layer. If you don’t define one, TensorFlow will auto-define an incremental one that makes it hard to test, as it will keep changing every time you train the model. When using a raw TensorFlow operation, you can’t assign a name to it. To address this issue, you’ll create a custom
layer that applies <code class="docutils literal notranslate"><span class="pre">reduce_mean</span></code> and call it <code class="docutils literal notranslate"><span class="pre">'classifier'</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class ReduceMeanLayer(tf.keras.layers.Layer):
  def __init__(self, axis=0, **kwargs):
    super(ReduceMeanLayer, self).__init__(**kwargs)
    self.axis = axis

  def call(self, input):
    return tf.math.reduce_mean(input, axis=self.axis)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>saved_model_path = &#39;./dogs_and_cats_yamnet&#39;

input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name=&#39;audio&#39;)
embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,
                                            trainable=False, name=&#39;yamnet&#39;)
_, embeddings_output, _ = embedding_extraction_layer(input_segment)
serving_outputs = my_model(embeddings_output)
serving_outputs = ReduceMeanLayer(axis=0, name=&#39;classifier&#39;)(serving_outputs)
serving_model = tf.keras.Model(input_segment, serving_outputs)
serving_model.save(saved_model_path, include_optimizer=False)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>tf.keras.utils.plot_model(serving_model)
</pre></div>
</div>
</div>
<p>Load your saved model to verify that it works as expected.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>reloaded_model = tf.saved_model.load(saved_model_path)
</pre></div>
</div>
</div>
<p>And for the final test: given some sound data, does your model return the correct result?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>reloaded_results = reloaded_model(testing_wav_data)
cat_or_dog = my_classes[tf.argmax(reloaded_results)]
print(f&#39;The main sound is: {cat_or_dog}&#39;)
</pre></div>
</div>
</div>
<p>If you want to try your new model on a serving setup, you can use the ‘serving_default’ signature.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>serving_results = reloaded_model.signatures[&#39;serving_default&#39;](testing_wav_data)
cat_or_dog = my_classes[tf.argmax(serving_results[&#39;classifier&#39;])]
print(f&#39;The main sound is: {cat_or_dog}&#39;)

</pre></div>
</div>
</div>
</div>
<div class="section" id="(Optional)-Some-more-testing">
<h3>(Optional) Some more testing<a class="headerlink" href="#(Optional)-Some-more-testing" title="Enlazar permanentemente con este título"></a></h3>
<p>The model is ready.</p>
<p>Let’s compare it to YAMNet on the test dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>test_pd = filtered_pd.loc[filtered_pd[&#39;fold&#39;] == 5]
row = test_pd.sample(1)
filename = row[&#39;filename&#39;].item()
print(filename)
waveform = load_wav_16k_mono(filename)
print(f&#39;Waveform values: {waveform}&#39;)
_ = plt.plot(waveform)

display.Audio(waveform, rate=16000)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Run the model, check the output.
scores, embeddings, spectrogram = yamnet_model(waveform)
class_scores = tf.reduce_mean(scores, axis=0)
top_class = tf.argmax(class_scores)
inferred_class = class_names[top_class]
top_score = class_scores[top_class]
print(f&#39;[YAMNet] The main sound is: {inferred_class} ({top_score})&#39;)

reloaded_results = reloaded_model(waveform)
your_top_class = tf.argmax(reloaded_results)
your_inferred_class = my_classes[your_top_class]
class_probabilities = tf.nn.softmax(reloaded_results, axis=-1)
your_top_score = class_probabilities[your_top_class]
print(f&#39;[Your model] The main sound is: {your_inferred_class} ({your_top_score})&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h3>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título"></a></h3>
<p>You have created a model that can classify sounds from dogs or cats. With the same idea and a different dataset you can try, for example, building an <a class="reference external" href="https://www.kaggle.com/c/birdclef-2021/">acoustic identifier of birds</a> based on their singing.</p>
<p>Share your project with the TensorFlow team on social media!</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>