<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Copyright 2020 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/deep_learning_09_tf_audio/1-01_simple_audio_recognition.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Simple-audio-recognition:-Recognizing-keywords">
<h2>Simple audio recognition: Recognizing keywords<a class="headerlink" href="#Simple-audio-recognition:-Recognizing-keywords" title="Enlazar permanentemente con este título"></a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><img alt="da889e9145cc421b843504fd87056b36" src="https://www.tensorflow.org/images/tf_logo_32px.png" /> View on TensorFlow.org</p>
</td><td><p><img alt="ffa0541b4a5a491c88594b04546daf25" src="https://www.tensorflow.org/images/colab_logo_32px.png" /> Run in Google Colab</p>
</td><td><p><img alt="bbe65c847ce54a169ca754bba5b6f2b5" src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" /> View source on GitHub</p>
</td><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>fa27df58cbdd4278acb8ec511757b295|Download notebook</p>
</td></table><p>This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic automatic speech recognition (ASR) model for recognizing ten different words. You will use a portion of the <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/speech_commands">Speech Commands dataset</a> (Warden, 2018), which contains short (one-second or less) audio clips of commands, such as “down”, “go”, “left”, “no”, “right”, “stop”, “up” and “yes”.</p>
<p>Real-world speech and audio recognition systems are complex. But, like <a class="reference external" href="../quickstart/beginner.ipynb">image classification with the MNIST dataset</a>, this tutorial should give you a basic understanding of the techniques involved.</p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título"></a></h3>
<p>Import necessary modules and dependencies. Note that you’ll be using seaborn for visualization in this tutorial.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import os
import pathlib

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display

# Set the seed value for experiment reproducibility.
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Import-the-mini-Speech-Commands-dataset">
<h3>Import the mini Speech Commands dataset<a class="headerlink" href="#Import-the-mini-Speech-Commands-dataset" title="Enlazar permanentemente con este título"></a></h3>
<p>To save time with data loading, you will be working with a smaller version of the Speech Commands dataset. The <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/speech_commands">original dataset</a> consists of over 105,000 audio files in the WAV (Waveform) audio file format of people saying 35 different words. This data was collected by Google and released under a CC BY license.</p>
<p>Download and extract the <code class="docutils literal notranslate"><span class="pre">mini_speech_commands.zip</span></code> file containing the smaller Speech Commands datasets with <code class="docutils literal notranslate"><span class="pre">tf.keras.utils.get_file</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DATASET_PATH = &#39;data/mini_speech_commands&#39;

data_dir = pathlib.Path(DATASET_PATH)
if not data_dir.exists():
  tf.keras.utils.get_file(
      &#39;mini_speech_commands.zip&#39;,
      origin=&quot;http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip&quot;,
      extract=True,
      cache_dir=&#39;.&#39;, cache_subdir=&#39;data&#39;)
</pre></div>
</div>
</div>
<p>The dataset’s audio clips are stored in eight folders corresponding to each speech command: <code class="docutils literal notranslate"><span class="pre">no</span></code>, <code class="docutils literal notranslate"><span class="pre">yes</span></code>, <code class="docutils literal notranslate"><span class="pre">down</span></code>, <code class="docutils literal notranslate"><span class="pre">go</span></code>, <code class="docutils literal notranslate"><span class="pre">left</span></code>, <code class="docutils literal notranslate"><span class="pre">up</span></code>, <code class="docutils literal notranslate"><span class="pre">right</span></code>, and <code class="docutils literal notranslate"><span class="pre">stop</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[commands != &#39;README.md&#39;]
print(&#39;Commands:&#39;, commands)
</pre></div>
</div>
</div>
<p>Extract the audio clips into a list called <code class="docutils literal notranslate"><span class="pre">filenames</span></code>, and shuffle it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>filenames = tf.io.gfile.glob(str(data_dir) + &#39;/*/*&#39;)
filenames = tf.random.shuffle(filenames)
num_samples = len(filenames)
print(&#39;Number of total examples:&#39;, num_samples)
print(&#39;Number of examples per label:&#39;,
      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))
print(&#39;Example file tensor:&#39;, filenames[0])
</pre></div>
</div>
</div>
<p>Split <code class="docutils literal notranslate"><span class="pre">filenames</span></code> into training, validation and test sets using a 80:10:10 ratio, respectively:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_files = filenames[:6400]
val_files = filenames[6400: 6400 + 800]
test_files = filenames[-800:]

print(&#39;Training set size&#39;, len(train_files))
print(&#39;Validation set size&#39;, len(val_files))
print(&#39;Test set size&#39;, len(test_files))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Read-the-audio-files-and-their-labels">
<h3>Read the audio files and their labels<a class="headerlink" href="#Read-the-audio-files-and-their-labels" title="Enlazar permanentemente con este título"></a></h3>
<p>In this section you will preprocess the dataset, creating decoded tensors for the waveforms and the corresponding labels. Note that:</p>
<ul class="simple">
<li><p>Each WAV file contains time-series data with a set number of samples per second.</p></li>
<li><p>Each sample represents the amplitude of the audio signal at that specific time.</p></li>
<li><p>In a 16-bit system, like the WAV files in the mini Speech Commands dataset, the amplitude values range from -32,768 to 32,767.</p></li>
<li><p>The sample rate for this dataset is 16kHz.</p></li>
</ul>
<p>The shape of the tensor returned by <code class="docutils literal notranslate"><span class="pre">tf.audio.decode_wav</span></code> is <code class="docutils literal notranslate"><span class="pre">[samples,</span> <span class="pre">channels]</span></code>, where <code class="docutils literal notranslate"><span class="pre">channels</span></code> is <code class="docutils literal notranslate"><span class="pre">1</span></code> for mono or <code class="docutils literal notranslate"><span class="pre">2</span></code> for stereo. The mini Speech Commands dataset only contains mono recordings.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>test_file = tf.io.read_file(DATASET_PATH+&#39;/down/0a9f9af7_nohash_0.wav&#39;)
test_audio, _ = tf.audio.decode_wav(contents=test_file)
test_audio.shape
</pre></div>
</div>
</div>
<p>Now, let’s define a function that preprocesses the dataset’s raw WAV audio files into audio tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def decode_audio(audio_binary):
  # Decode WAV-encoded audio files to `float32` tensors, normalized
  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.
  audio, _ = tf.audio.decode_wav(contents=audio_binary)
  # Since all the data is single channel (mono), drop the `channels`
  # axis from the array.
  return tf.squeeze(audio, axis=-1)
</pre></div>
</div>
</div>
<p>Define a function that creates labels using the parent directories for each file:</p>
<ul class="simple">
<li><p>Split the file paths into <code class="docutils literal notranslate"><span class="pre">tf.RaggedTensor</span></code>s (tensors with ragged dimensions—with slices that may have different lengths).</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_label(file_path):
  parts = tf.strings.split(
      input=file_path,
      sep=os.path.sep)
  # Note: You&#39;ll use indexing here instead of tuple unpacking to enable this
  # to work in a TensorFlow graph.
  return parts[-2]
</pre></div>
</div>
</div>
<p>Define another helper function—<code class="docutils literal notranslate"><span class="pre">get_waveform_and_label</span></code>—that puts it all together:</p>
<ul class="simple">
<li><p>The input is the WAV audio filename.</p></li>
<li><p>The output is a tuple containing the audio and label tensors ready for supervised learning.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_waveform_and_label(file_path):
  label = get_label(file_path)
  audio_binary = tf.io.read_file(file_path)
  waveform = decode_audio(audio_binary)
  return waveform, label
</pre></div>
</div>
</div>
<p>Build the training set to extract the audio-label pairs:</p>
<ul class="simple">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code> with <code class="docutils literal notranslate"><span class="pre">Dataset.from_tensor_slices</span></code> and <code class="docutils literal notranslate"><span class="pre">Dataset.map</span></code>, using <code class="docutils literal notranslate"><span class="pre">get_waveform_and_label</span></code> defined earlier.</p></li>
</ul>
<p>You’ll build the validation and test sets using a similar procedure later on.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>AUTOTUNE = tf.data.AUTOTUNE

files_ds = tf.data.Dataset.from_tensor_slices(train_files)

waveform_ds = files_ds.map(
    map_func=get_waveform_and_label,
    num_parallel_calls=AUTOTUNE)
</pre></div>
</div>
</div>
<p>Let’s plot a few audio waveforms:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rows = 3
cols = 3
n = rows * cols
fig, axes = plt.subplots(rows, cols, figsize=(10, 12))

for i, (audio, label) in enumerate(waveform_ds.take(n)):
  r = i // cols
  c = i % cols
  ax = axes[r][c]
  ax.plot(audio.numpy())
  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))
  label = label.numpy().decode(&#39;utf-8&#39;)
  ax.set_title(label)

plt.show()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Convert-waveforms-to-spectrograms">
<h3>Convert waveforms to spectrograms<a class="headerlink" href="#Convert-waveforms-to-spectrograms" title="Enlazar permanentemente con este título"></a></h3>
<p>The waveforms in the dataset are represented in the time domain. Next, you’ll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the short-time Fourier transform (STFT) to convert the waveforms to as spectrograms, which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.</p>
<p>A Fourier transform (<code class="docutils literal notranslate"><span class="pre">tf.signal.fft</span></code>) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (<code class="docutils literal notranslate"><span class="pre">tf.signal.stft</span></code>) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.</p>
<p>Create a utility function for converting waveforms to spectrograms:</p>
<ul class="simple">
<li><p>The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using <code class="docutils literal notranslate"><span class="pre">tf.zeros</span></code>).</p></li>
<li><p>When calling <code class="docutils literal notranslate"><span class="pre">tf.signal.stft</span></code>, choose the <code class="docutils literal notranslate"><span class="pre">frame_length</span></code> and <code class="docutils literal notranslate"><span class="pre">frame_step</span></code> parameters such that the generated spectrogram “image” is almost square. For more information on the STFT parameters choice, refer to this Coursera video on audio signal processing and STFT.</p></li>
<li><p>The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you’ll only use the magnitude, which you can derive by applying <code class="docutils literal notranslate"><span class="pre">tf.abs</span></code> on the output of <code class="docutils literal notranslate"><span class="pre">tf.signal.stft</span></code>.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_spectrogram(waveform):
  # Zero-padding for an audio waveform with less than 16,000 samples.
  input_len = 16000
  waveform = waveform[:input_len]
  zero_padding = tf.zeros(
      [16000] - tf.shape(waveform),
      dtype=tf.float32)
  # Cast the waveform tensors&#39; dtype to float32.
  waveform = tf.cast(waveform, dtype=tf.float32)
  # Concatenate the waveform with `zero_padding`, which ensures all audio
  # clips are of the same length.
  equal_length = tf.concat([waveform, zero_padding], 0)
  # Convert the waveform to a spectrogram via a STFT.
  spectrogram = tf.signal.stft(
      equal_length, frame_length=255, frame_step=128)
  # Obtain the magnitude of the STFT.
  spectrogram = tf.abs(spectrogram)
  # Add a `channels` dimension, so that the spectrogram can be used
  # as image-like input data with convolution layers (which expect
  # shape (`batch_size`, `height`, `width`, `channels`).
  spectrogram = spectrogram[..., tf.newaxis]
  return spectrogram
</pre></div>
</div>
</div>
<p>Next, start exploring the data. Print the shapes of one example’s tensorized waveform and the corresponding spectrogram, and play the original audio:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for waveform, label in waveform_ds.take(1):
  label = label.numpy().decode(&#39;utf-8&#39;)
  spectrogram = get_spectrogram(waveform)

print(&#39;Label:&#39;, label)
print(&#39;Waveform shape:&#39;, waveform.shape)
print(&#39;Spectrogram shape:&#39;, spectrogram.shape)
print(&#39;Audio playback&#39;)
display.display(display.Audio(waveform, rate=16000))
</pre></div>
</div>
</div>
<p>Now, define a function for displaying a spectrogram:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def plot_spectrogram(spectrogram, ax):
  if len(spectrogram.shape) &gt; 2:
    assert len(spectrogram.shape) == 3
    spectrogram = np.squeeze(spectrogram, axis=-1)
  # Convert the frequencies to log scale and transpose, so that the time is
  # represented on the x-axis (columns).
  # Add an epsilon to avoid taking a log of zero.
  log_spec = np.log(spectrogram.T + np.finfo(float).eps)
  height = log_spec.shape[0]
  width = log_spec.shape[1]
  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)
  Y = range(height)
  ax.pcolormesh(X, Y, log_spec)
</pre></div>
</div>
</div>
<p>Plot the example’s waveform over time and the corresponding spectrogram (frequencies over time):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>fig, axes = plt.subplots(2, figsize=(12, 8))
timescale = np.arange(waveform.shape[0])
axes[0].plot(timescale, waveform.numpy())
axes[0].set_title(&#39;Waveform&#39;)
axes[0].set_xlim([0, 16000])

plot_spectrogram(spectrogram.numpy(), axes[1])
axes[1].set_title(&#39;Spectrogram&#39;)
plt.show()
</pre></div>
</div>
</div>
<p>Now, define a function that transforms the waveform dataset into spectrograms and their corresponding labels as integer IDs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def get_spectrogram_and_label_id(audio, label):
  spectrogram = get_spectrogram(audio)
  label_id = tf.argmax(label == commands)
  return spectrogram, label_id
</pre></div>
</div>
</div>
<p>Map <code class="docutils literal notranslate"><span class="pre">get_spectrogram_and_label_id</span></code> across the dataset’s elements with <code class="docutils literal notranslate"><span class="pre">Dataset.map</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>spectrogram_ds = waveform_ds.map(
  map_func=get_spectrogram_and_label_id,
  num_parallel_calls=AUTOTUNE)
</pre></div>
</div>
</div>
<p>Examine the spectrograms for different examples of the dataset:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>rows = 3
cols = 3
n = rows*cols
fig, axes = plt.subplots(rows, cols, figsize=(10, 10))

for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):
  r = i // cols
  c = i % cols
  ax = axes[r][c]
  plot_spectrogram(spectrogram.numpy(), ax)
  ax.set_title(commands[label_id.numpy()])
  ax.axis(&#39;off&#39;)

plt.show()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Build-and-train-the-model">
<h3>Build and train the model<a class="headerlink" href="#Build-and-train-the-model" title="Enlazar permanentemente con este título"></a></h3>
<p>Repeat the training set preprocessing on the validation and test sets:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def preprocess_dataset(files):
  files_ds = tf.data.Dataset.from_tensor_slices(files)
  output_ds = files_ds.map(
      map_func=get_waveform_and_label,
      num_parallel_calls=AUTOTUNE)
  output_ds = output_ds.map(
      map_func=get_spectrogram_and_label_id,
      num_parallel_calls=AUTOTUNE)
  return output_ds
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_ds = spectrogram_ds
val_ds = preprocess_dataset(val_files)
test_ds = preprocess_dataset(test_files)
</pre></div>
</div>
</div>
<p>Batch the training and validation sets for model training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>batch_size = 64
train_ds = train_ds.batch(batch_size)
val_ds = val_ds.batch(batch_size)
</pre></div>
</div>
</div>
<p>Add <code class="docutils literal notranslate"><span class="pre">Dataset.cache</span></code> and <code class="docutils literal notranslate"><span class="pre">Dataset.prefetch</span></code> operations to reduce read latency while training the model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_ds = train_ds.cache().prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)
</pre></div>
</div>
</div>
<p>For the model, you’ll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.</p>
<p>Your <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential</span></code> model will use the following Keras preprocessing layers:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Resizing</span></code>: to downsample the input to enable the model to train faster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Normalization</span></code>: to normalize each pixel in the image based on its mean and standard deviation.</p></li>
</ul>
<p>For the <code class="docutils literal notranslate"><span class="pre">Normalization</span></code> layer, its <code class="docutils literal notranslate"><span class="pre">adapt</span></code> method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for spectrogram, _ in spectrogram_ds.take(1):
  input_shape = spectrogram.shape
print(&#39;Input shape:&#39;, input_shape)
num_labels = len(commands)

# Instantiate the `tf.keras.layers.Normalization` layer.
norm_layer = layers.Normalization()
# Fit the state of the layer to the spectrograms
# with `Normalization.adapt`.
norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))

model = models.Sequential([
    layers.Input(shape=input_shape),
    # Downsample the input.
    layers.Resizing(32, 32),
    # Normalize.
    norm_layer,
    layers.Conv2D(32, 3, activation=&#39;relu&#39;),
    layers.Conv2D(64, 3, activation=&#39;relu&#39;),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(128, activation=&#39;relu&#39;),
    layers.Dropout(0.5),
    layers.Dense(num_labels),
])

model.summary()
</pre></div>
</div>
</div>
<p>Configure the Keras model with the Adam optimizer and the cross-entropy loss:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[&#39;accuracy&#39;],
)
</pre></div>
</div>
</div>
<p>Train the model over 10 epochs for demonstration purposes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>EPOCHS = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),
)
</pre></div>
</div>
</div>
<p>Let’s plot the training and validation loss curves to check how your model has improved during training:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>metrics = history.history
plt.plot(history.epoch, metrics[&#39;loss&#39;], metrics[&#39;val_loss&#39;])
plt.legend([&#39;loss&#39;, &#39;val_loss&#39;])
plt.show()
</pre></div>
</div>
</div>
</div>
<div class="section" id="Evaluate-the-model-performance">
<h3>Evaluate the model performance<a class="headerlink" href="#Evaluate-the-model-performance" title="Enlazar permanentemente con este título"></a></h3>
<p>Run the model on the test set and check the model’s performance:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>test_audio = []
test_labels = []

for audio, label in test_ds:
  test_audio.append(audio.numpy())
  test_labels.append(label.numpy())

test_audio = np.array(test_audio)
test_labels = np.array(test_labels)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>y_pred = np.argmax(model.predict(test_audio), axis=1)
y_true = test_labels

test_acc = sum(y_pred == y_true) / len(y_true)
print(f&#39;Test set accuracy: {test_acc:.0%}&#39;)
</pre></div>
</div>
</div>
<div class="section" id="Display-a-confusion-matrix">
<h4>Display a confusion matrix<a class="headerlink" href="#Display-a-confusion-matrix" title="Enlazar permanentemente con este título"></a></h4>
<p>Use a confusion matrix to check how well the model did classifying each of the commands in the test set:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_mtx,
            xticklabels=commands,
            yticklabels=commands,
            annot=True, fmt=&#39;g&#39;)
plt.xlabel(&#39;Prediction&#39;)
plt.ylabel(&#39;Label&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Run-inference-on-an-audio-file">
<h3>Run inference on an audio file<a class="headerlink" href="#Run-inference-on-an-audio-file" title="Enlazar permanentemente con este título"></a></h3>
<p>Finally, verify the model’s prediction output using an input audio file of someone saying “no”. How well does your model perform?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sample_file = data_dir/&#39;no/01bb6a2a_nohash_0.wav&#39;

sample_ds = preprocess_dataset([str(sample_file)])

for spectrogram, label in sample_ds.batch(1):
  prediction = model(spectrogram)
  plt.bar(commands, tf.nn.softmax(prediction[0]))
  plt.title(f&#39;Predictions for &quot;{commands[label[0]]}&quot;&#39;)
  plt.show()
</pre></div>
</div>
</div>
<p>As the output suggests, your model should have recognized the audio command as “no”.</p>
</div>
<div class="section" id="Next-steps">
<h3>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título"></a></h3>
<p>This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://www.tensorflow.org/hub/tutorials/yamnet">Sound classification with YAMNet</a> tutorial shows how to use transfer learning for audio classification.</p></li>
<li><p>The notebooks from Kaggle’s TensorFlow speech recognition challenge.</p></li>
<li><p>The TensorFlow.js - Audio recognition using transfer learning codelab teaches how to build your own interactive web app for audio classification.</p></li>
<li><p>A tutorial on deep learning for music information retrieval (Choi et al., 2017) on arXiv.</p></li>
<li><p>TensorFlow also has additional support for <a class="reference external" href="https://www.tensorflow.org/io/tutorials/audio">audio data preparation and augmentation</a> to help with your own audio-based projects.</p></li>
<li><p>Consider using the librosa library—a Python package for music and audio analysis.</p></li>
</ul>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>