<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2021 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" aria-label="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Asignaturas de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Asignaturas de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_descriptiva/index.html">Analítica descriptiva y visualización de datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Copyright 2021 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/tensorflow_11_model_understanding/1-02_sngp.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Copyright-2021-The-TensorFlow-Authors.">
<h1>Copyright 2021 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2021-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<section id="Uncertainty-aware-Deep-Learning-with-SNGP">
<h2>Uncertainty-aware Deep Learning with SNGP<a class="headerlink" href="#Uncertainty-aware-Deep-Learning-with-SNGP" title="Enlazar permanentemente con este título"></a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>41548bf86ad44e79bb72a71c0a8957a0|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>01e879739ba7408f879b95da1980e962|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>c7872cdf1aab48c4ba7a7920a8b970fb|View on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>6bae86f0bf29483793253647b8520341|Download notebook</p>
</td></table><p>In AI applications that are safety-critical (e.g., medical decision making and autonomous driving) or where the data is inherently noisy (e.g., natural language understanding), it is important for a deep classifier to reliably quantify its uncertainty. The deep classifier should be able to be aware of its own limitations and when it should hand control over to the human experts. This tutorial shows how to improve a deep classifier’s ability in quantifying uncertainty using a technique called
<strong>Spectral-normalized Neural Gaussian Process (</strong><a class="reference external" href="https://arxiv.org/abs/2006.10108">SNGP</a><strong>)</strong>.</p>
<p>The core idea of SNGP is to improve a deep classifier’s <strong>distance awareness</strong> by applying simple modifications to the network. A model’s <em>distance awareness</em> is a measure of how its predictive probability reflects the distance between the test example and the training data. This is a desirable property that is common for gold-standard probablistic models (e.g., the <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian process</a> with RBF kernels) but is lacking in models with deep neural
networks. SNGP provides a simple way to inject this Gaussian-process behavior into a deep classifier while maintaining its predictive accuracy.</p>
<p>This tutorial implements a deep residual network (ResNet)-based SNGP model on the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html">two moons</a> dataset, and compares its uncertainty surface with that of two other popular uncertainty approaches - <a class="reference external" href="https://arxiv.org/abs/1506.02142">Monte Carlo dropout</a> and <a class="reference external" href="https://arxiv.org/abs/1612.01474">Deep ensemble</a>).</p>
<p>This tutorial illustrates the SNGP model on a toy 2D dataset. For an example of applying SNGP to a real-world natural language understanding task using BERT-base, check out the <a class="reference external" href="https://www.tensorflow.org/text/tutorials/uncertainty_quantification_with_sngp_bert">SNGP-BERT tutorial</a>. For high-quality implementations of an SNGP model (and many other uncertainty methods) on a wide variety of benchmark datasets (such as <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/cifar100">CIFAR-100</a>,
<a class="reference external" href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet</a>, <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/wikipedia_toxicity_subtypes">Jigsaw toxicity detection</a>, etc), refer to the <a class="reference external" href="https://github.com/google/uncertainty-baselines">Uncertainty Baselines</a> benchmark.</p>
<section id="About-SNGP">
<h3>About SNGP<a class="headerlink" href="#About-SNGP" title="Enlazar permanentemente con este título"></a></h3>
<p><a class="reference external" href="https://arxiv.org/abs/2006.10108">Spectral-normalized Neural Gaussian Process</a> (SNGP) is a simple approach to improve a deep classifier’s uncertainty quality while maintaining a similar level of accuracy and latency. Given a deep residual network, SNGP makes two simple changes to the model:</p>
<ul class="simple">
<li><p>It applies spectral normalization to the hidden residual layers.</p></li>
<li><p>It replaces the Dense output layer with a Gaussian process layer.</p></li>
</ul>
<blockquote>
<div><p><img alt="SNGP" src="http://tensorflow.org/tutorials/understanding/images/sngp.png" /></p>
</div></blockquote>
<p>Compared to other uncertainty approaches (e.g., Monte Carlo dropout or Deep ensemble), SNGP has several advantages:</p>
<ul class="simple">
<li><p>It works for a wide range of state-of-the-art residual-based architectures (e.g., (Wide) ResNet, DenseNet, BERT, etc).</p></li>
<li><p>It is a single-model method (i.e., does not rely on ensemble averaging). Therefore SNGP has a similar level of latency as a single deterministic network, and can be scaled easily to large datasets like <a class="reference external" href="https://github.com/google/uncertainty-baselines/tree/main/baselines/imagenet">ImageNet</a> and <a class="reference external" href="https://github.com/google/uncertainty-baselines/tree/main/baselines/toxic_comments">Jigsaw Toxic Comments classification</a>.</p></li>
<li><p>It has strong out-of-domain detection performance due to the <em>distance-awareness</em> property.</p></li>
</ul>
<p>The downsides of this method are:</p>
<ul class="simple">
<li><p>The predictive uncertainty of a SNGP is computed using the <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW3.pdf">Laplace approximation</a>. Therefore theoretically, the posterior uncertainty of SNGP is different from that of an exact Gaussian process.</p></li>
<li><p>SNGP training needs a covariance reset step at the begining of a new epoch. This can add a tiny amount of extra complexity to a training pipeline. This tutorial shows a simple way to implement this using Keras callbacks.</p></li>
</ul>
</section>
<section id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>!pip install --use-deprecated=legacy-resolver tf-models-official
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># refresh pkg_resources so it takes the changes into account.
import pkg_resources
import importlib
importlib.reload(pkg_resources)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import matplotlib.colors as colors

import sklearn.datasets

import numpy as np
import tensorflow as tf

import official.nlp.modeling.layers as nlp_layers
</pre></div>
</div>
</div>
<p>Define visualization macros</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.rcParams[&#39;figure.dpi&#39;] = 140

DEFAULT_X_RANGE = (-3.5, 3.5)
DEFAULT_Y_RANGE = (-2.5, 2.5)
DEFAULT_CMAP = colors.ListedColormap([&quot;#377eb8&quot;, &quot;#ff7f00&quot;])
DEFAULT_NORM = colors.Normalize(vmin=0, vmax=1,)
DEFAULT_N_GRID = 100
</pre></div>
</div>
</div>
</section>
<section id="The-two-moon-dataset">
<h3>The two moon dataset<a class="headerlink" href="#The-two-moon-dataset" title="Enlazar permanentemente con este título"></a></h3>
<p>Create the trainining and evaluation datasets from the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html">two moon dataset</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def make_training_data(sample_size=500):
  &quot;&quot;&quot;Create two moon training dataset.&quot;&quot;&quot;
  train_examples, train_labels = sklearn.datasets.make_moons(
      n_samples=2 * sample_size, noise=0.1)

  # Adjust data position slightly.
  train_examples[train_labels == 0] += [-0.1, 0.2]
  train_examples[train_labels == 1] += [0.1, -0.2]

  return train_examples, train_labels
</pre></div>
</div>
</div>
<p>Evaluate the model’s predictive behavior over the entire 2D input space.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def make_testing_data(x_range=DEFAULT_X_RANGE, y_range=DEFAULT_Y_RANGE, n_grid=DEFAULT_N_GRID):
  &quot;&quot;&quot;Create a mesh grid in 2D space.&quot;&quot;&quot;
  # testing data (mesh grid over data space)
  x = np.linspace(x_range[0], x_range[1], n_grid)
  y = np.linspace(y_range[0], y_range[1], n_grid)
  xv, yv = np.meshgrid(x, y)
  return np.stack([xv.flatten(), yv.flatten()], axis=-1)
</pre></div>
</div>
</div>
<p>To evaluate model uncertainty, add an out-of-domain (OOD) dataset that belongs to a third class. The model never sees these OOD examples during training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def make_ood_data(sample_size=500, means=(2.5, -1.75), vars=(0.01, 0.01)):
  return np.random.multivariate_normal(
      means, cov=np.diag(vars), size=sample_size)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Load the train, test and OOD datasets.
train_examples, train_labels = make_training_data(
    sample_size=500)
test_examples = make_testing_data()
ood_examples = make_ood_data(sample_size=500)

# Visualize
pos_examples = train_examples[train_labels == 0]
neg_examples = train_examples[train_labels == 1]

plt.figure(figsize=(7, 5.5))

plt.scatter(pos_examples[:, 0], pos_examples[:, 1], c=&quot;#377eb8&quot;, alpha=0.5)
plt.scatter(neg_examples[:, 0], neg_examples[:, 1], c=&quot;#ff7f00&quot;, alpha=0.5)
plt.scatter(ood_examples[:, 0], ood_examples[:, 1], c=&quot;red&quot;, alpha=0.1)

plt.legend([&quot;Postive&quot;, &quot;Negative&quot;, &quot;Out-of-Domain&quot;])

plt.ylim(DEFAULT_Y_RANGE)
plt.xlim(DEFAULT_X_RANGE)

plt.show()
</pre></div>
</div>
</div>
<p>Here the blue and orange represent the positive and negative classes, and the red represents the OOD data. A model that quantifies the uncertainty well is expected to be confident when close to training data (i.e., <span class="math notranslate nohighlight">\(p(x_{test})\)</span> close to 0 or 1), and be uncertain when far away from the training data regions (i.e., <span class="math notranslate nohighlight">\(p(x_{test})\)</span> close to 0.5).</p>
</section>
<section id="The-deterministic-model">
<h3>The deterministic model<a class="headerlink" href="#The-deterministic-model" title="Enlazar permanentemente con este título"></a></h3>
<section id="Define-model">
<h4>Define model<a class="headerlink" href="#Define-model" title="Enlazar permanentemente con este título"></a></h4>
<p>Start from the (baseline) deterministic model: a multi-layer residual network (ResNet) with dropout regularization.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>#@title
class DeepResNet(tf.keras.Model):
  &quot;&quot;&quot;Defines a multi-layer residual network.&quot;&quot;&quot;
  def __init__(self, num_classes, num_layers=3, num_hidden=128,
               dropout_rate=0.1, **classifier_kwargs):
    super().__init__()
    # Defines class meta data.
    self.num_hidden = num_hidden
    self.num_layers = num_layers
    self.dropout_rate = dropout_rate
    self.classifier_kwargs = classifier_kwargs

    # Defines the hidden layers.
    self.input_layer = tf.keras.layers.Dense(self.num_hidden, trainable=False)
    self.dense_layers = [self.make_dense_layer() for _ in range(num_layers)]

    # Defines the output layer.
    self.classifier = self.make_output_layer(num_classes)

  def call(self, inputs):
    # Projects the 2d input data to high dimension.
    hidden = self.input_layer(inputs)

    # Computes the resnet hidden representations.
    for i in range(self.num_layers):
      resid = self.dense_layers[i](hidden)
      resid = tf.keras.layers.Dropout(self.dropout_rate)(resid)
      hidden += resid

    return self.classifier(hidden)

  def make_dense_layer(self):
    &quot;&quot;&quot;Uses the Dense layer as the hidden layer.&quot;&quot;&quot;
    return tf.keras.layers.Dense(self.num_hidden, activation=&quot;relu&quot;)

  def make_output_layer(self, num_classes):
    &quot;&quot;&quot;Uses the Dense layer as the output layer.&quot;&quot;&quot;
    return tf.keras.layers.Dense(
        num_classes, **self.classifier_kwargs)
</pre></div>
</div>
</div>
<p>This tutorial uses a 6-layer ResNet with 128 hidden units.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_config = dict(num_classes=2, num_layers=6, num_hidden=128)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_model = DeepResNet(**resnet_config)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_model.build((None, 2))
resnet_model.summary()
</pre></div>
</div>
</div>
</section>
<section id="Train-model">
<h4>Train model<a class="headerlink" href="#Train-model" title="Enlazar permanentemente con este título"></a></h4>
<p>Configure the training parameters to use <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy</span></code> as the loss function and the Adam optimizer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = tf.keras.metrics.SparseCategoricalAccuracy(),
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)
</pre></div>
</div>
</div>
<p>Train the model for 100 epochs with batch size 128.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>fit_config = dict(batch_size=128, epochs=100)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_model.compile(**train_config)
resnet_model.fit(train_examples, train_labels, **fit_config)
</pre></div>
</div>
</div>
</section>
<section id="Visualize-uncertainty">
<h4>Visualize uncertainty<a class="headerlink" href="#Visualize-uncertainty" title="Enlazar permanentemente con este título"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>#@title
def plot_uncertainty_surface(test_uncertainty, ax, cmap=None):
  &quot;&quot;&quot;Visualizes the 2D uncertainty surface.

  For simplicity, assume these objects already exist in the memory:

    test_examples: Array of test examples, shape (num_test, 2).
    train_labels: Array of train labels, shape (num_train, ).
    train_examples: Array of train examples, shape (num_train, 2).

  Arguments:
    test_uncertainty: Array of uncertainty scores, shape (num_test,).
    ax: A matplotlib Axes object that specifies a matplotlib figure.
    cmap: A matplotlib colormap object specifying the palette of the
      predictive surface.

  Returns:
    pcm: A matplotlib PathCollection object that contains the palette
      information of the uncertainty plot.
  &quot;&quot;&quot;
  # Normalize uncertainty for better visualization.
  test_uncertainty = test_uncertainty / np.max(test_uncertainty)

  # Set view limits.
  ax.set_ylim(DEFAULT_Y_RANGE)
  ax.set_xlim(DEFAULT_X_RANGE)

  # Plot normalized uncertainty surface.
  pcm = ax.imshow(
      np.reshape(test_uncertainty, [DEFAULT_N_GRID, DEFAULT_N_GRID]),
      cmap=cmap,
      origin=&quot;lower&quot;,
      extent=DEFAULT_X_RANGE + DEFAULT_Y_RANGE,
      vmin=DEFAULT_NORM.vmin,
      vmax=DEFAULT_NORM.vmax,
      interpolation=&#39;bicubic&#39;,
      aspect=&#39;auto&#39;)

  # Plot training data.
  ax.scatter(train_examples[:, 0], train_examples[:, 1],
             c=train_labels, cmap=DEFAULT_CMAP, alpha=0.5)
  ax.scatter(ood_examples[:, 0], ood_examples[:, 1], c=&quot;red&quot;, alpha=0.1)

  return pcm
</pre></div>
</div>
</div>
<p>Now visualize the predictions of the deterministic model. First plot the class probability:</p>
<div class="math notranslate nohighlight">
\[p(x) = softmax(logit(x))\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_logits = resnet_model(test_examples)
resnet_probs = tf.nn.softmax(resnet_logits, axis=-1)[:, 0]  # Take the probability for class 0.
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>_, ax = plt.subplots(figsize=(7, 5.5))

pcm = plot_uncertainty_surface(resnet_probs, ax=ax)

plt.colorbar(pcm, ax=ax)
plt.title(&quot;Class Probability, Deterministic Model&quot;)

plt.show()
</pre></div>
</div>
</div>
<p>In this plot, the yellow and purple are the predictive probabilities for the two classes. The deterministic model did a good job in classifying the two known classes (blue and orange) with a nonlinear decision boundary. However, it is not <strong>distance-aware</strong>, and classified the never-seen red out-of-domain (OOD) examples confidently as the orange class.</p>
<p>Visualize the model uncertainty by computing the <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution#Variance">predictive variance</a>:</p>
<div class="math notranslate nohighlight">
\[var(x) = p(x) * (1 - p(x))\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_uncertainty = resnet_probs * (1 - resnet_probs)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>_, ax = plt.subplots(figsize=(7, 5.5))

pcm = plot_uncertainty_surface(resnet_uncertainty, ax=ax)

plt.colorbar(pcm, ax=ax)
plt.title(&quot;Predictive Uncertainty, Deterministic Model&quot;)

plt.show()
</pre></div>
</div>
</div>
<p>In this plot, the yellow indicates high uncertainty, and the purple indicates low uncertainty. A deterministic ResNet’s uncertainty depends only on the test examples’ distance from the decision boundary. This leads the model to be over-confident when out of the training domain. The next section shows how SNGP behaves differently on this dataset.</p>
</section>
</section>
<section id="The-SNGP-model">
<h3>The SNGP model<a class="headerlink" href="#The-SNGP-model" title="Enlazar permanentemente con este título"></a></h3>
<section id="Define-SNGP-model">
<h4>Define SNGP model<a class="headerlink" href="#Define-SNGP-model" title="Enlazar permanentemente con este título"></a></h4>
<p>Let’s now implement the SNGP model. Both the SNGP components, <code class="docutils literal notranslate"><span class="pre">SpectralNormalization</span></code> and <code class="docutils literal notranslate"><span class="pre">RandomFeatureGaussianProcess</span></code>, are available at the tensorflow_model’s <a class="reference external" href="https://github.com/tensorflow/models/tree/master/official/nlp/modeling/layers">built-in layers</a>.</p>
<blockquote>
<div><p><img alt="SNGP" src="http://tensorflow.org/tutorials/understanding/images/sngp.png" /></p>
</div></blockquote>
<p>Let’s look at these two components in more detail. (You can also jump to the <a class="reference external" href="#full-sngp-model">The SNGP model</a> section to see how the full model is implemented.)</p>
<section id="Spectral-Normalization-wrapper">
<h5>Spectral Normalization wrapper<a class="headerlink" href="#Spectral-Normalization-wrapper" title="Enlazar permanentemente con este título"></a></h5>
<p><code class="docutils literal notranslate"><span class="pre">`SpectralNormalization</span></code> &lt;<a class="reference external" href="https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/spectral_normalization.py">https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/spectral_normalization.py</a>&gt;`__ is a Keras layer wrapper. It can be applied to an existing Dense layer like this:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>dense = tf.keras.layers.Dense(units=10)
dense = nlp_layers.SpectralNormalization(dense, norm_multiplier=0.9)
</pre></div>
</div>
</div>
<p>Spectral normalization regularizes the hidden weight <span class="math notranslate nohighlight">\(W\)</span> by gradually guiding its spectral norm (i.e., the largest eigenvalue of <span class="math notranslate nohighlight">\(W\)</span>) toward the target value <code class="docutils literal notranslate"><span class="pre">norm_multiplier</span></code>.</p>
<p>Note: Usually it is preferable to set <code class="docutils literal notranslate"><span class="pre">norm_multiplier</span></code> to a value smaller than 1. However in practice, it can be also relaxed to a larger value to ensure the deep network has enough expressive power.</p>
</section>
<section id="The-Gaussian-Process-(GP)-layer">
<h5>The Gaussian Process (GP) layer<a class="headerlink" href="#The-Gaussian-Process-(GP)-layer" title="Enlazar permanentemente con este título"></a></h5>
<p><code class="docutils literal notranslate"><span class="pre">`RandomFeatureGaussianProcess</span></code> &lt;<a class="reference external" href="https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/gaussian_process.py">https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/gaussian_process.py</a>&gt;`__ implements a <a class="reference external" href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">random-feature based approximation</a> to a Gaussian process model that is end-to-end trainable with a deep neural network. Under the hood, the Gaussian process layer implements a two-layer network:</p>
<div class="math notranslate nohighlight">
\[logits(x) = \Phi(x) \beta, \quad \Phi(x)=\sqrt{\frac{2}{M}} * cos(Wx + b)\]</div>
<p>Here <span class="math notranslate nohighlight">\(x\)</span> is the input, and <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are frozen weights initialized randomly from Gaussian and uniform distributions, respectively. (Therefore <span class="math notranslate nohighlight">\(\Phi(x)\)</span> are called “random features”.) <span class="math notranslate nohighlight">\(\beta\)</span> is the learnable kernel weight similar to that of a Dense layer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>batch_size = 32
input_dim = 1024
num_classes = 10
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>gp_layer = nlp_layers.RandomFeatureGaussianProcess(units=num_classes,
                                               num_inducing=1024,
                                               normalize_input=False,
                                               scale_random_features=True,
                                               gp_cov_momentum=-1)
</pre></div>
</div>
</div>
<p>The main parameters of the GP layers are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">units</span></code>: The dimension of the output logits.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_inducing</span></code>: The dimension <span class="math notranslate nohighlight">\(M\)</span> of the hidden weight <span class="math notranslate nohighlight">\(W\)</span>. Default to 1024.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">normalize_input</span></code>: Whether to apply layer normalization to the input <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scale_random_features</span></code>: Whether to apply the scale <span class="math notranslate nohighlight">\(\sqrt{2/M}\)</span> to the hidden output.</p></li>
</ul>
<p>Note: For a deep neural network that is sensitive to the learning rate (e.g., ResNet-50 and ResNet-110), it is generally recommended to set <code class="docutils literal notranslate"><span class="pre">normalize_input=True</span></code> to stablize training, and set <code class="docutils literal notranslate"><span class="pre">scale_random_features=False</span></code> to avoid the learning rate from being modified in unexpected ways when passing through the GP layer.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gp_cov_momentum</span></code> controls how the model covariance is computed. If set to a positive value (e.g., 0.999), the covariance matrix is computed using the momentum-based moving average update (similar to batch normalization). If set to -1, the the covariance matrix is updated without momentum.</p></li>
</ul>
<p>Note: The momentum-based update method can be sensitive to batch size. Therefore it is generally recommended to set <code class="docutils literal notranslate"><span class="pre">gp_cov_momentum=-1</span></code> to compute the covariance exactly. For this to work properly, the covariance matrix estimator needs to be reset at the begining of a new epoch in order to avoid counting the same data twice. For <code class="docutils literal notranslate"><span class="pre">RandomFeatureGaussianProcess</span></code>, this is can be done by calling its <code class="docutils literal notranslate"><span class="pre">reset_covariance_matrix()</span></code>. The next section shows an easy implementation of this using Keras’
built-in API.</p>
<p>Given a batch input with shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_dim)</span></code>, the GP layer returns a <code class="docutils literal notranslate"><span class="pre">logits</span></code> tensor (shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_classes)</span></code>) for prediction, and also <code class="docutils literal notranslate"><span class="pre">covmat</span></code> tensor (shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">batch_size)</span></code>) which is the posterior covariance matrix of the batch logits.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>embedding = tf.random.normal(shape=(batch_size, input_dim))

logits, covmat = gp_layer(embedding)
</pre></div>
</div>
</div>
<p>Note: Notice that under this implementation of the SNGP model, the predictive logits <span class="math notranslate nohighlight">\(logit(x_{test})\)</span> for all classes share the same covariance matrix <span class="math notranslate nohighlight">\(var(x_{test})\)</span>, which describes the distance between <span class="math notranslate nohighlight">\(x_{test}\)</span> from the training data.</p>
<p>Theoretically, it is possible to extend the algorithm to compute different variance values for different classes (as introduced in the <a class="reference external" href="https://arxiv.org/abs/2006.10108">original SNGP paper</a>). However, this is difficult to scale to problems with large output spaces (e.g., ImageNet or language modeling).</p>
<p>#### The full SNGP model</p>
<p>Given the base class <code class="docutils literal notranslate"><span class="pre">DeepResNet</span></code>, the SNGP model can be implemented easily by modifying the residual network’s hidden and output layers. For compatibility with Keras <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> API, also modify the model’s <code class="docutils literal notranslate"><span class="pre">call()</span></code> method so it only outputs <code class="docutils literal notranslate"><span class="pre">logits</span></code> during training.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>class DeepResNetSNGP(DeepResNet):
  def __init__(self, spec_norm_bound=0.9, **kwargs):
    self.spec_norm_bound = spec_norm_bound
    super().__init__(**kwargs)

  def make_dense_layer(self):
    &quot;&quot;&quot;Applies spectral normalization to the hidden layer.&quot;&quot;&quot;
    dense_layer = super().make_dense_layer()
    return nlp_layers.SpectralNormalization(
        dense_layer, norm_multiplier=self.spec_norm_bound)

  def make_output_layer(self, num_classes):
    &quot;&quot;&quot;Uses Gaussian process as the output layer.&quot;&quot;&quot;
    return nlp_layers.RandomFeatureGaussianProcess(
        num_classes,
        gp_cov_momentum=-1,
        **self.classifier_kwargs)

  def call(self, inputs, training=False, return_covmat=False):
    # Gets logits and covariance matrix from GP layer.
    logits, covmat = super().call(inputs)

    # Returns only logits during training.
    if not training and return_covmat:
      return logits, covmat

    return logits
</pre></div>
</div>
</div>
<p>Use the same architecture as the deterministic model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>resnet_config
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_model = DeepResNetSNGP(**resnet_config)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_model.build((None, 2))
sngp_model.summary()
</pre></div>
</div>
</div>
<p>Implement a Keras callback to reset the covariance matrix at the beginning of a new epoch.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>class ResetCovarianceCallback(tf.keras.callbacks.Callback):

  def on_epoch_begin(self, epoch, logs=None):
    &quot;&quot;&quot;Resets covariance matrix at the begining of the epoch.&quot;&quot;&quot;
    if epoch &gt; 0:
      self.model.classifier.reset_covariance_matrix()
</pre></div>
</div>
</div>
<p>Add this callback to the <code class="docutils literal notranslate"><span class="pre">DeepResNetSNGP</span></code> model class.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>class DeepResNetSNGPWithCovReset(DeepResNetSNGP):
  def fit(self, *args, **kwargs):
    &quot;&quot;&quot;Adds ResetCovarianceCallback to model callbacks.&quot;&quot;&quot;
    kwargs[&quot;callbacks&quot;] = list(kwargs.get(&quot;callbacks&quot;, []))
    kwargs[&quot;callbacks&quot;].append(ResetCovarianceCallback())

    return super().fit(*args, **kwargs)
</pre></div>
</div>
</div>
</section>
</section>
<section id="id9">
<h4>Train model<a class="headerlink" href="#id9" title="Enlazar permanentemente con este título"></a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">tf.keras.model.fit</span></code> to train the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_model = DeepResNetSNGPWithCovReset(**resnet_config)
sngp_model.compile(**train_config)
sngp_model.fit(train_examples, train_labels, **fit_config)
</pre></div>
</div>
</div>
</section>
<section id="id10">
<h4>Visualize uncertainty<a class="headerlink" href="#id10" title="Enlazar permanentemente con este título"></a></h4>
<p>First compute the predictive logits and variances.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_variance = tf.linalg.diag_part(sngp_covmat)[:, None]
</pre></div>
</div>
</div>
<p>Now compute the posterior predictive probability. The classic method for computing the predictive probability of a probabilistic model is to use Monte Carlo sampling, i.e.,</p>
<div class="math notranslate nohighlight">
\[E(p(x)) = \frac{1}{M} \sum_{m=1}^M logit_m(x),\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the sample size, and <span class="math notranslate nohighlight">\(logit_m(x)\)</span> are random samples from the SNGP posterior <span class="math notranslate nohighlight">\(MultivariateNormal\)</span>(<code class="docutils literal notranslate"><span class="pre">sngp_logits</span></code>,<code class="docutils literal notranslate"><span class="pre">sngp_covmat</span></code>). However, this approach can be slow for latency-sensitive applications such as autonomous driving or real-time bidding. Instead, can approximate <span class="math notranslate nohighlight">\(E(p(x))\)</span> using the <a class="reference external" href="https://arxiv.org/abs/2006.07584">mean-field method</a>:</p>
<div class="math notranslate nohighlight">
\[E(p(x)) \approx softmax(\frac{logit(x)}{\sqrt{1+ \lambda * \sigma^2(x)}})\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2(x)\)</span> is the SNGP variance, and <span class="math notranslate nohighlight">\(\lambda\)</span> is often chosen as <span class="math notranslate nohighlight">\(\pi/8\)</span> or <span class="math notranslate nohighlight">\(3/\pi^2\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_logits_adjusted = sngp_logits / tf.sqrt(1. + (np.pi / 8.) * sngp_variance)
sngp_probs = tf.nn.softmax(sngp_logits_adjusted, axis=-1)[:, 0]
</pre></div>
</div>
</div>
<p>Note: Instead of fixing <span class="math notranslate nohighlight">\(\lambda\)</span> to a fixed value, you can also treat it as a hyperparameter, and tune it to optimize the model’s calibration performance. This is known as <a class="reference external" href="http://proceedings.mlr.press/v70/guo17a.html">temperature scaling</a> in the deep learning uncertainty literature.</p>
<p>This mean-field method is implemented as a built-in function <code class="docutils literal notranslate"><span class="pre">layers.gaussian_process.mean_field_logits</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def compute_posterior_mean_probability(logits, covmat, lambda_param=np.pi / 8.):
  # Computes uncertainty-adjusted logits using the built-in method.
  logits_adjusted = nlp_layers.gaussian_process.mean_field_logits(
      logits, covmat, mean_field_factor=lambda_param)

  return tf.nn.softmax(logits_adjusted, axis=-1)[:, 0]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)
sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)
</pre></div>
</div>
</div>
</section>
<section id="SNGP-Summary">
<h4>SNGP Summary<a class="headerlink" href="#SNGP-Summary" title="Enlazar permanentemente con este título"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>#@title

def plot_predictions(pred_probs, model_name=&quot;&quot;):
  &quot;&quot;&quot;Plot normalized class probabilities and predictive uncertainties.&quot;&quot;&quot;
  # Compute predictive uncertainty.
  uncertainty = pred_probs * (1. - pred_probs)

  # Initialize the plot axes.
  fig, axs = plt.subplots(1, 2, figsize=(14, 5))

  # Plots the class probability.
  pcm_0 = plot_uncertainty_surface(pred_probs, ax=axs[0])
  # Plots the predictive uncertainty.
  pcm_1 = plot_uncertainty_surface(uncertainty, ax=axs[1])

  # Adds color bars and titles.
  fig.colorbar(pcm_0, ax=axs[0])
  fig.colorbar(pcm_1, ax=axs[1])

  axs[0].set_title(f&quot;Class Probability, {model_name}&quot;)
  axs[1].set_title(f&quot;(Normalized) Predictive Uncertainty, {model_name}&quot;)

  plt.show()
</pre></div>
</div>
</div>
<p>Put everything together. The entire procedure (training, evaluation and uncertainty computation) can be done in just five lines:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def train_and_test_sngp(train_examples, test_examples):
  sngp_model = DeepResNetSNGPWithCovReset(**resnet_config)

  sngp_model.compile(**train_config)
  sngp_model.fit(train_examples, train_labels, verbose=0, **fit_config)

  sngp_logits, sngp_covmat = sngp_model(test_examples, return_covmat=True)
  sngp_probs = compute_posterior_mean_probability(sngp_logits, sngp_covmat)

  return sngp_probs
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sngp_probs = train_and_test_sngp(train_examples, test_examples)
</pre></div>
</div>
</div>
<p>Visualize the class probability (left) and the predictive uncertainty (right) of the SNGP model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plot_predictions(sngp_probs, model_name=&quot;SNGP&quot;)
</pre></div>
</div>
</div>
<p>Remember that in the class probability plot (left), the yellow and purple are class probabilities. When close to the training data domain, SNGP correctly classifies the examples with high confidence (i.e., assigning near 0 or 1 probability). When far away from the training data, SNGP gradually becomes less confident, and its predictive probability becomes close to 0.5 while the (normalized) model uncertainty rises to 1.</p>
<p>Compare this to the uncertainty surface of the deterministic model:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plot_predictions(resnet_probs, model_name=&quot;Deterministic&quot;)
</pre></div>
</div>
</div>
<p>Like mentioned earlier, a deterministic model is not <em>distance-aware</em>. Its uncertainty is defined by the distance of the test example from the decision boundary. This leads the model to produce overconfident predictions for the out-of-domain examples (red).</p>
</section>
</section>
<section id="Comparison-with-other-uncertainty-approaches">
<h3>Comparison with other uncertainty approaches<a class="headerlink" href="#Comparison-with-other-uncertainty-approaches" title="Enlazar permanentemente con este título"></a></h3>
<p>This section compares the uncertainty of SNGP with <a class="reference external" href="https://arxiv.org/abs/1506.02142">Monte Carlo dropout</a> and <a class="reference external" href="https://arxiv.org/abs/1612.01474">Deep ensemble</a>.</p>
<p>Both of these methods are based on Monte Carlo averaging of multiple forward passes of deterministic models. First set the ensemble size <span class="math notranslate nohighlight">\(M\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>num_ensemble = 10
</pre></div>
</div>
</div>
<section id="Monte-Carlo-dropout">
<h4>Monte Carlo dropout<a class="headerlink" href="#Monte-Carlo-dropout" title="Enlazar permanentemente con este título"></a></h4>
<p>Given a trained neural network with Dropout layers, <a class="reference external" href="https://arxiv.org/abs/1506.02142">Monte Carlo dropout</a> computes the mean predictive probability</p>
<div class="math notranslate nohighlight">
\[E(p(x)) = \frac{1}{M}\sum_{m=1}^M softmax(logit_m(x))\]</div>
<p>by averaging over multiple Dropout-enabled forward passes <span class="math notranslate nohighlight">\(\{logit_m(x)\}_{m=1}^M\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def mc_dropout_sampling(test_examples):
  # Enable dropout during inference.
  return resnet_model(test_examples, training=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Monte Carlo dropout inference.
dropout_logit_samples = [mc_dropout_sampling(test_examples) for _ in range(num_ensemble)]
dropout_prob_samples = [tf.nn.softmax(dropout_logits, axis=-1)[:, 0] for dropout_logits in dropout_logit_samples]
dropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>dropout_probs = tf.reduce_mean(dropout_prob_samples, axis=0)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plot_predictions(dropout_probs, model_name=&quot;MC Dropout&quot;)
</pre></div>
</div>
</div>
</section>
<section id="Deep-ensemble">
<h4>Deep ensemble<a class="headerlink" href="#Deep-ensemble" title="Enlazar permanentemente con este título"></a></h4>
<p><a class="reference external" href="https://arxiv.org/abs/1612.01474">Deep ensemble</a> is a state-of-the-art (but expensive) method for deep learning uncertainty. To train a Deep ensemble, first train <span class="math notranslate nohighlight">\(M\)</span> ensemble members.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Deep ensemble training
resnet_ensemble = []
for _ in range(num_ensemble):
  resnet_model = DeepResNet(**resnet_config)
  resnet_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
  resnet_model.fit(train_examples, train_labels, verbose=0, **fit_config)

  resnet_ensemble.append(resnet_model)
</pre></div>
</div>
</div>
<p>Collect logits and compute the mean predctive probability <span class="math notranslate nohighlight">\(E(p(x)) = \frac{1}{M}\sum_{m=1}^M softmax(logit_m(x))\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Deep ensemble inference
ensemble_logit_samples = [model(test_examples) for model in resnet_ensemble]
ensemble_prob_samples = [tf.nn.softmax(logits, axis=-1)[:, 0] for logits in ensemble_logit_samples]
ensemble_probs = tf.reduce_mean(ensemble_prob_samples, axis=0)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plot_predictions(ensemble_probs, model_name=&quot;Deep ensemble&quot;)
</pre></div>
</div>
</div>
<p>Both MC Dropout and Deep ensemble improve a model’s uncertainty ability by making the decision boundary less certain. However, they both inherit the deterministic deep network’s limitation in lacking distance awareness.</p>
</section>
</section>
<section id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Enlazar permanentemente con este título"></a></h3>
<p>In this tutorial, you have: * Implemented a SNGP model on a deep classifier to improve its distance awareness. * Trained the SNGP model end-to-end using Keras <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> API. * Visualized the uncertainty behavior of SNGP. * Compared the uncertainty behavior between SNGP, Monte Carlo dropout and deep ensemble models.</p>
</section>
<section id="Resources-and-further-reading">
<h3>Resources and further reading<a class="headerlink" href="#Resources-and-further-reading" title="Enlazar permanentemente con este título"></a></h3>
<ul class="simple">
<li><p>See the <a class="reference external" href="https://www.tensorflow.org/text/tutorials/uncertainty_quantification_with_sngp_bert">SNGP-BERT tutorial</a> for an example of applying SNGP on a BERT model for uncertainty-aware natural language understanding.</p></li>
<li><p>See <a class="reference external" href="https://github.com/google/uncertainty-baselines">Uncertainty Baselines</a> for the implementation of SNGP model (and many other uncertainty methods) on a wide variety of benchmark datasets (e.g., <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/cifar100">CIFAR</a>, <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet</a>, <a class="reference external" href="https://www.tensorflow.org/datasets/catalog/wikipedia_toxicity_subtypes">Jigsaw toxicity detection</a>, etc).</p></li>
<li><p>For a deeper understanding of the SNGP method, check out the paper <a class="reference external" href="https://arxiv.org/abs/2006.10108">Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness</a>.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>