<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lectura: Reconocimiento de patrones binarios usando neuronas de McCulloch-Pitts — 30:00 min &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="Lectura: Perceptrones binarios — 45:00 min" href="../binary_perceptron/1-01-perceptron-binario.html" />
    <link rel="prev" title="Algoritmos para modificar la tasa de aprendizaje — 9:22" href="../optimization_for_ML/1-07_algoritmos_para_modificar_la_tasa_de_aprendizaje.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-01-2022-mar-08">Sesión 01 — 2022-mar-08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-02-2022-mar-14">Sesión 02 — 2022-mar-14</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-03-2022-mar-22">Sesión 03 — 2022-mar-22</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-04-2022-mar-29">Sesión 04 — 2022-mar-29</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-05-2022-abr-05">Sesión 05 — 2022-abr-05</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-06-2022-abr-19">Sesión 06 — 2022-abr-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-07-2022-abr-26">Sesión 07 — 2022-abr-26</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-08-2022-may-03">Sesión 08 — 2022-may-03</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-09-2022-may-10">Sesión 09 — 2022-may-10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-10-2022-may-17">Sesión 10 — 2022-may-17</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-11-2022-may-24">Sesión 11 — 2022-may-24</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-12-2022-may-31">Sesión 12 — 2022-may-31</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-13-2022-jun-07">Sesión 13 — 2022-jun-07</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-14-2022-jun-14">Sesión 14 — 2022-jun-14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-15-2022-jun-21">Sesión 15 — 2022-jun-21</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#sesion-16-2022-jun-28">Sesión 16 — 2022-jun-28</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html#material-para-proximos-cursos-2023">Material para próximos cursos (2023)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a> &raquo;</li>
      <li>Lectura: Reconocimiento de patrones binarios usando neuronas de McCulloch-Pitts — 30:00 min</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/mcculloch-pitts_neuron/1-01-McCullochPitts.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Lectura:-Reconocimiento-de-patrones-binarios-usando-neuronas-de-McCulloch-Pitts-—-30:00-min">
<h1>Lectura: Reconocimiento de patrones binarios usando neuronas de McCulloch-Pitts — 30:00 min<a class="headerlink" href="#Lectura:-Reconocimiento-de-patrones-binarios-usando-neuronas-de-McCulloch-Pitts-—-30:00-min" title="Enlazar permanentemente con este título"></a></h1>
<ul class="simple">
<li><p>Última modificación: Marzo 10, 2022</p></li>
</ul>
<div class="section" id="Definición-del-problema">
<h2>Definición del problema<a class="headerlink" href="#Definición-del-problema" title="Enlazar permanentemente con este título"></a></h2>
<p>El problema real abordado por McCulloch y Pitts consistía en desarrollar un sistema de visión que permite identificar patrones binarios simples. Es decir, asociar el patrón de entrada (0, 0, 1) a 1, o el (1, 1, 1) a 0.</p>
<p>En términos de los datos, se tiene un conjunto de cuatro patrones binarios de entrada que deben ser reconocidos (que aparecen en la figura de abajo), donde cada patrón está conformado por tres dígitos binarios <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>. Cada patrón binario es representado como una columna con cajones, y los valores binarios están codificados con los colores blanco y gris. En este caso, el sistema de visión debe determinar el valor de los tres bits (entrada al modelo) y el cerebro debe determinar si el
patrón arbitrario observado corresponde a uno de los cuatro patrones indicados (decisión).</p>
<p>En términos matemáticos, este problema puede ser definido como un problema de clasificación de patrones donde las entradas son todas las cadenas de tres dígitos binarios posibles, y la salida es 1 si la cadena es reconocida y 0 en caso contrario. Dicho de otra forma, los todos los patrones binarios podrían ser asignados de forma única a dos conjuntos posibles, y el modelo determina a cual conjunto {0, 1} pertenece el patrón.</p>
<p><img alt="assets/McCullochPitts-01.png" src="../../_images/McCullochPitts-01.png" /></p>
<p>En otras palabras, se desea tener un modelo matemático <span class="math notranslate nohighlight">\(f(\)</span> <code class="docutils literal notranslate"><span class="pre">Entrada</span></code> <span class="math notranslate nohighlight">\()\)</span> = <code class="docutils literal notranslate"><span class="pre">Salida</span></code> cuyas entradas y salidas están determinadas por la siguiente tabla, donde cero es blanco y uno es gris. Los patrones con salida 1 son los que deben ser reconocidos.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Entrada   Salida
------------------
   000       0
   001       1
   010       0
   011       0
   100       1
   101       0
   110       1
   111       1
</pre></div>
</div>
</div>
<div class="section" id="Solución">
<h2>Solución<a class="headerlink" href="#Solución" title="Enlazar permanentemente con este título"></a></h2>
<div class="section" id="Modelo-matemático-de-la-neurona">
<h3>Modelo matemático de la neurona<a class="headerlink" href="#Modelo-matemático-de-la-neurona" title="Enlazar permanentemente con este título"></a></h3>
<p>El modelo de neurona de <a class="reference external" href="https://en.wikipedia.org/wiki/Artificial_neuron">McCulloch-Pitts</a> fue propuesto originalmente como un postulado sobre la forma en que el cerebro puede reconocer patrones complejos (parte derecha de la figura anterior). Este modelo plantea que, en general, una célula (neurona) puede representarse matemáticamente como una función no lineal que es descrita a continuación.</p>
<p>El modelo de la neurona se basa en una unidad genérica de cómputo que aparece en la figura de abajo. La neurona (unidad de cómputo) recibe varias entradas binarias excitatorias notadas como <span class="math notranslate nohighlight">\(x_i\)</span>; la neurona agrega estas entradas mediante la función <span class="math notranslate nohighlight">\(g()\)</span>, definida usualmente como (parte izquierda de la figura de abajo):</p>
<div class="math notranslate nohighlight">
\[g(x_1, ...,x_n) = v = \sum_{i=1}^n x_i\]</div>
<p>para obtener una entrada neta <span class="math notranslate nohighlight">\(v\)</span>. Posteriormente la entrada neta <span class="math notranslate nohighlight">\(v\)</span> es transformada con una función no lineal <span class="math notranslate nohighlight">\(f()\)</span> definida como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(v) =
\begin{cases}
      1, &amp; \text{Si $v \ge \theta$}\\
      0, &amp; \text{Si $v \lt \theta$}\\
\end{cases}\end{split}\]</div>
<p>El valor <span class="math notranslate nohighlight">\(\theta\)</span> es un umbral. Así, la salida de la neurona es un dígito binario <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> (parte central de la figura de abajo).</p>
<p><img alt="assets/McCullochPitts-02.png" src="../../_images/McCullochPitts-02.png" /></p>
<p>Por ejemplo, para el primer patrón de la tabla de la primera figura (000), la entrada neta será 0 (= 0 + 0 + 0), mientras que para el último patrón (111( será 3 (= 1 + 1 + 1). Si se tiene que parar dicha neurona el <span class="math notranslate nohighlight">\(\theta\)</span> es igual a 2, la salida para el patron 000, será 0 (0 &lt; 2) y para el último 1 (3 &gt; 2).</p>
<p>Adicionalmente, la neurona artificial contiene conexiones inhibitorias notadas como <span class="math notranslate nohighlight">\(y_m\)</span>, tal que la salida siempre es cero si alguna de las entradas inhibitorias vale 1, independientemente del valor que puedan tomar las conexiones excitatorias. La representación gráfica de la neurona de McCulloch-Pitts aparece en la parte derecha de la figura anterior.</p>
<p>Las entradas <span class="math notranslate nohighlight">\(x_i\)</span> son señales exitadores, y las señales <span class="math notranslate nohighlight">\(y_j\)</span> son inhibidoras. La salida es cero (0) si alguna de las señales inhibidoras es uno (1). La salida es uno (1) si la suma de señales de entrada es mayor o igual que el umbral (<span class="math notranslate nohighlight">\(\theta\)</span>), y todas las señales inhibidoras son cero (0).</p>
<p>Una neurona de McCulloch-Pitts puede interpretarse como una compuerta lógica de umbral (circuito lógico) amplicamente conocida en electrónica:</p>
<p><img alt="assets/McCullochPitts-06.png" src="../../_images/McCullochPitts-06.png" /></p>
<p>Por ejemplo:</p>
<ul class="simple">
<li><p>Para <span class="math notranslate nohighlight">\(x_1\)</span> AND <span class="math notranslate nohighlight">\(x_2\)</span>:</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> x1   x2     x1 AND x2    v   Umbral    Salida
             (deseada)                v &gt;= umbral
---------------------------------------------------
 0    0         0         0     2          0
 0    1         0         1     2          0
 1    0         0         1     2          0
 1    1         1         2     2          1
</pre></div>
</div>
<ul class="simple">
<li><p>Para <span class="math notranslate nohighlight">\(x_1\)</span> NOR <span class="math notranslate nohighlight">\(x_2\)</span>:</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> x1   x2     x1 NOR x2    v   Umbral    Salida
             (deseada)                v &gt;= umbral
---------------------------------------------------
 0    0         1         0     0          1
 0    1         0     x1 es inhibitoria    0
 1    0         0     x2 es inhibitoria    0
 1    1         0  x1, x2 son inhibitorias 0
</pre></div>
</div>
<p><strong>Ejercicio.—</strong> Calcule la salida para la siguiente red de neuronas de McCulloch-Pitts.</p>
<p><img alt="assets/McCullochPitts-04.png" src="../../_images/McCullochPitts-04.png" /></p>
<p>Rta/</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Entrada  Salida
-----------------
  0 0 0     0
  0 0 1     0
  0 1 0     0
  0 1 1     1
  1 0 0     0
  1 0 1     0
  1 1 0     1
  1 1 1     1
</pre></div>
</div>
</div>
<div class="section" id="Representación-del-problema-de-clasificación-como-una-función-lógica">
<h3>Representación del problema de clasificación como una función lógica<a class="headerlink" href="#Representación-del-problema-de-clasificación-como-una-función-lógica" title="Enlazar permanentemente con este título"></a></h3>
<p>El problema de clasificación planteado inicialmente, puede ser resuelto mediante la construcción de una función lógica <span class="math notranslate nohighlight">\(y=f(x_1, x_2, ..., x_n)\)</span> definida como <span class="math notranslate nohighlight">\(f:\{0,1\}^n \to \{0,1\}\)</span> con: <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span>, y <span class="math notranslate nohighlight">\(x_i \in \{0,1\}\)</span>. En otras palabras, una función lógica es una función <span class="math notranslate nohighlight">\(f\)</span> que recibe como entrada una cadena de bits de tamaño <span class="math notranslate nohighlight">\(n\)</span> o <span class="math notranslate nohighlight">\(\{0,1\}^n\)</span> y devuelve un dígito binario <span class="math notranslate nohighlight">\(\{0,1\}\)</span>. En términos del problema planteado, las entradas a la
función lógica son las cadenas de bits observadas y la salida en un número binario que indica que el patrón se reconoce o no. Para el problema abordado, la función lógica requerida es definida por la siguiente tabla:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> x1 x2 x3   f
---------------
  0  0  0   0
  0  0  1   1
  0  1  0   0
  0  1  1   0
  1  0  0   1
  1  0  1   0
  1  1  0   1
  1  1  1   1
</pre></div>
</div>
</div>
<div class="section" id="Representación-de-funciones-lógicas-mediante-redes-de-neuronas-de-McCulloch-Pitts">
<h3>Representación de funciones lógicas mediante redes de neuronas de McCulloch-Pitts<a class="headerlink" href="#Representación-de-funciones-lógicas-mediante-redes-de-neuronas-de-McCulloch-Pitts" title="Enlazar permanentemente con este título"></a></h3>
<p>Una red de neuronas de McCulloch-Pitts de dos capas puede representar cualquier función lógica <span class="math notranslate nohighlight">\(F:\{0,1\}^n \to \{0,1\}\)</span>. A continuación se presenta el proceso de construcción para la función lógica que se presenta en la siguiente figura:</p>
<p><img alt="assets/McCullochPitts-03.png" src="../../_images/McCullochPitts-03.png" /></p>
<ul class="simple">
<li><p>Se crea una neurona en la primera capa por cada salida igual a 1; para el ejemplo planteado se requieren dos neuronas.</p></li>
<li><p>La segunda capa contiene una neurona que representa la función OR; esto es, si todas las entradas a la neurona son cero, la salida es cero; si una o más entradas son uno, la salida de la neurona es uno. Esta neurona recibe como entrada todas las salida de las neuronas de la primera capa; todas las entradas son excitatorias y el umbral es 1.</p></li>
<li><p>Cada neurona de entrada se especializa en un patrón binario de entrada así: si una entrada es cero, la correspondiente conexión se hace inhibitoria y excitatoria en caso contrario; por ejemplo, para el patrón de entrada 001 (primera neurona de la primera capa) las conexiones para <span class="math notranslate nohighlight">\(x_1\)</span> y <span class="math notranslate nohighlight">\(x2\)</span> son inhibitorias y la conexión para <span class="math notranslate nohighlight">\(x_3\)</span> es excitatoria; y para el patrón 010, las conexiones correspondientes a <span class="math notranslate nohighlight">\(x_1\)</span> y <span class="math notranslate nohighlight">\(x_3\)</span> son inhibitorias y para <span class="math notranslate nohighlight">\(x_2\)</span>
excitatoria. El valor del umbral de la neurona es la cantidad de unos de la entrada. Así para los patrones 001 y 010 el umbral es 1.</p></li>
</ul>
</div>
<div class="section" id="Representación-matricial-de-la-operación-de-una-red-de-neuronas">
<h3>Representación matricial de la operación de una red de neuronas<a class="headerlink" href="#Representación-matricial-de-la-operación-de-una-red-de-neuronas" title="Enlazar permanentemente con este título"></a></h3>
<p>Para la implementación, las conexiones entre las neuronas son representadas mediante matrices. Si las conexiones inhibitorias son representas por un número negativo grande <span class="math notranslate nohighlight">\(N\)</span>, las conexiones a la primera capa de procesamiento puede representarse como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{w} =
\begin{bmatrix}
 N &amp; N &amp; 1 \\
 N &amp; 1 &amp; N
\end{bmatrix}\end{split}\]</div>
<p>donde las filas representan las neuronas y las columnas los dígitos binarios de la entrada.</p>
<p>De esta forma, la entrada neta para el patron 001 es:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
 N &amp; N &amp; 1 \\
 N &amp; 1 &amp; N
\end{bmatrix}
\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ N \end{bmatrix}\end{split}\]</div>
<p>En la práctica resulta más conveniente usar un vector para representar los umbrales de las neuronas (que en este caso sería un vector de unos), tal que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix} 1 \\ N \end{bmatrix} -
\begin{bmatrix} 1 \\ 1 \end{bmatrix} =
\begin{bmatrix} 0 \\ N-1 \end{bmatrix}\end{split}\]</div>
<p>Seguidamente, se aplica la función de transformación no lineal <span class="math notranslate nohighlight">\(f()\)</span>. El resultado obtenido corresponde a la salida de las dos neuronas de la primera capa de procesamiento.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f \left(\begin{bmatrix} 0 \\ N-1 \end{bmatrix} \right) =
\begin{bmatrix} f(0) \\ f(N-1) \end{bmatrix} =
\begin{bmatrix} 1 \\ 0 \end{bmatrix}\end{split}\]</div>
<p>que fue definida anteriormente como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(v) =
\begin{cases}
      1, &amp; \text{Si $v \ge \theta$}\\
      0, &amp; \text{Si $v \lt \theta$}\\
\end{cases}\end{split}\]</div>
<p>Recuerde que <span class="math notranslate nohighlight">\(N\)</span> es un número negativo muy grande, tal que <span class="math notranslate nohighlight">\(f()\)</span> siempre se evalua a cero.</p>
<p>Finalmente, la función OR que representa neurona de salida puede ser computada con la función vectorial <span class="math notranslate nohighlight">\(\max()\)</span>, la cual devuelve el valor máximo de su argumento.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\max \left(\begin{bmatrix} 1 \\ 0 \end{bmatrix} \right) = \max(1, 0) = 1\end{split}\]</div>
<p>Este es el proceso de cálculo que se implementa computacionalmente.</p>
</div>
<div class="section" id="Solución-al-problema-propuesto">
<h3>Solución al problema propuesto<a class="headerlink" href="#Solución-al-problema-propuesto" title="Enlazar permanentemente con este título"></a></h3>
<p>Para el problema propuesto, cada patrón puede ser codificado como un vector de tres posiciones. Cuando el cuadro es negro, el valor de la posición correspondiente del vector es +1 y cuando es blanco es 0. Cada patrón es asociado a una variable de salida que toma el valor de +1 cuando el patrón debe ser reconocido y 0 cuando debe ser ignorado. De esta forma, el problema puede plantearse como:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    Entrada    Salida
 (x1, x2, x3)                 +----+----+----+
-----------------------       | x1 | x2 | x3 |
      000        0            +----+----+----+
      001        1
      010        0
      011        0
      100        1
      101        0
      110        1
      111        1
</pre></div>
</div>
<p>De esta forma, el patrón 100 se representaría matricialmente como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x} =
\begin{bmatrix}
 1 \\
 0 \\
 0
\end{bmatrix}\end{split}\]</div>
</div>
</div>
<div class="section" id="Implementación-en-Python">
<h2>Implementación en Python<a class="headerlink" href="#Implementación-en-Python" title="Enlazar permanentemente con este título"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">Layer</span><span class="p">:</span>
    <span class="c1">#</span>
    <span class="c1"># Se implementa una clase genérica de capa. Al agregar varias capas se</span>
    <span class="c1"># obtiene una red neuronal.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># ---------------------------------------------------------------------</span>
        <span class="c1"># Número de neuronas de salida de la capa</span>
        <span class="n">units</span><span class="p">,</span>
        <span class="c1"># ---------------------------------------------------------------------</span>
        <span class="c1"># Número de neuronas de entrada a la capa</span>
        <span class="n">input_dim</span><span class="p">,</span>
        <span class="c1"># ---------------------------------------------------------------------</span>
        <span class="c1"># Función de activación de la capa (esta corresponde a la función</span>
        <span class="c1"># umbral)</span>
        <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="c1"># ---------------------------------------------------------------------</span>
        <span class="c1"># Semilla para la generación de pesos aleatorios de las conexiones.</span>
        <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># Se inicializacn los parámetros de la capa</span>
        <span class="c1">#</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1">#</span>
        <span class="c1"># Si el usuario no especifica una semilla para el generador aleatorio,</span>
        <span class="c1"># se usa el valor por defecto del sistemas</span>
        <span class="c1">#</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># Se verifica si la activación es None y se asigna la función identidad</span>
        <span class="c1">#</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1">#</span>
            <span class="c1"># La función de activación puede ser especificada como un string</span>
            <span class="c1">#</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)),</span>
                <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span>
                <span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">check_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># Si los pesos no han sido inicializados se asignan valores aleatorios.</span>
        <span class="c1"># Este es el procedimiento normal en modelos de redes neuronales.</span>
        <span class="c1">#</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
                <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
                <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># La capa puede llamarse como si fuese una función. Primero, se</span>
        <span class="c1"># verifican los pesos y luego se hace el cálculo de la propagación de</span>
        <span class="c1"># la señal</span>
        <span class="c1">#</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_weights</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_activation</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OrLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="c1">#</span>
    <span class="c1"># Se implementa una capa especializada para representar la función OR.</span>
    <span class="c1"># Note que se usa la clase general Layer como base y solo se redefinen</span>
    <span class="c1"># las funciones que cambian.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">units</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">McCullochPittsNetwork</span><span class="p">:</span>
    <span class="c1">#</span>
    <span class="c1"># Esta clase representa una red neuronal de McCullochPitts con una capa</span>
    <span class="c1"># oculta y una capa de ssalida con una única neurona.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># X es una lista de listas.</span>
        <span class="c1"># y es una lista</span>
        <span class="c1">#</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># Se copia X para evitar efectos colaterles y se extraen únicamente los</span>
        <span class="c1"># patrones para los cuales y == 1.</span>
        <span class="c1">#</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1">#</span>
        <span class="c1"># La cantidad de entradas a la red es igual a la cantidad de columas de</span>
        <span class="c1"># la matriz de ejemplos.</span>
        <span class="c1">#</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1">#</span>
        <span class="c1"># La capa oculta tiene tantas neuronas como 1s exitan en y.</span>
        <span class="c1">#</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span>
            <span class="n">units</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;step&quot;</span>
        <span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># Aquí se computa el theta para cada neurona de la capa oculta. Note</span>
        <span class="c1"># que axis=1, indica que se hace la suma por filas para la matriz X.</span>
        <span class="c1"># Es decir, el theta es igual a la cantidad de 1s que hay en el patrón</span>
        <span class="c1"># de entrada.</span>
        <span class="c1">#</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># En este segmento de código se computa el N que aparecen en las</span>
        <span class="c1"># ecuaciones. Para el patron 001, la salida debe ser 1, y para los</span>
        <span class="c1"># patrones 101, 011 y 111 la salida de esta neurona debe ser 0.</span>
        <span class="c1">#</span>
        <span class="c1"># Recuerde que hay otra neurona especializada en el patron 111 del</span>
        <span class="c1"># ejemplo.</span>
        <span class="c1">#</span>
        <span class="c1"># Para lograr esto, los pesos deben ser [-4, -4, 1], teniendo en</span>
        <span class="c1"># cuenta que -4 = -3 (cantidad de columnas) - 1</span>
        <span class="c1">#</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">w</span>

        <span class="c1">#</span>
        <span class="c1"># La capa de salida de la red es una capa con una sola neurona OR</span>
        <span class="c1">#</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">OrLayer</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># Aca simplemente se imprime la representación en texto de la red</span>
        <span class="c1"># neuronal</span>
        <span class="c1">#</span>
        <span class="k">def</span> <span class="nf">coef2string</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">isvar</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">coef</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">coef</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">isvar</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="k">return</span> <span class="s2">&quot; + &quot;</span>
                <span class="k">return</span> <span class="s2">&quot; + </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">coef</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">coef</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">isvar</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="k">return</span> <span class="s2">&quot; - &quot;</span>
                <span class="k">return</span> <span class="s2">&quot; - </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">-</span><span class="n">coef</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">var2string</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">coef</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">coef2string</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;x</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span>

        <span class="n">text_hidden</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">neuron</span><span class="p">,</span> <span class="n">bias</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">bias</span>
        <span class="p">):</span>
            <span class="n">eq</span> <span class="o">=</span> <span class="p">[</span><span class="n">var2string</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">neuron</span><span class="p">)]</span>
            <span class="k">if</span> <span class="n">bias</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">eq</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eq</span><span class="p">)</span> <span class="o">+</span> <span class="n">coef2string</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eq</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eq</span><span class="p">)</span>

            <span class="n">eq</span> <span class="o">=</span> <span class="n">eq</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">eq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;+&quot;</span><span class="p">:</span>
                <span class="n">eq</span> <span class="o">=</span> <span class="n">eq</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">text_hidden</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;step(&quot;</span> <span class="o">+</span> <span class="n">eq</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">)</span>

        <span class="n">text_output</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">coef2string</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">t</span>
            <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">bias</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">text_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coef2string</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="kc">False</span><span class="p">))</span>

        <span class="n">text_output</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;    &quot;</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">text_output</span><span class="p">]</span>
        <span class="n">text_output</span> <span class="o">=</span> <span class="s2">&quot;step(</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_output</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span>

        <span class="k">return</span> <span class="n">text_output</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Ejemplo propuesto</span>
<span class="c1">#</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">]</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1">#</span>
<span class="c1"># Se crea la red neuronal</span>
<span class="c1">#</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">McCullochPittsNetwork</span><span class="p">()</span>

<span class="c1">#</span>
<span class="c1"># Se realiza el entrenamiento</span>
<span class="c1">#</span>
<span class="n">nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Se infiere para un conjunto de datos de entrada</span>
<span class="c1">#</span>
<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[0],
       [1],
       [0],
       [0],
       [1],
       [0],
       [1],
       [1]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0, 1, 0, 0, 1, 0, 1, 1])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Valores de las conexiones de la capa de entrada a la capa oculta</span>
<span class="c1">#</span>
<span class="n">nn</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">kernel</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[-4,  1,  1,  1],
       [-4, -4,  1,  1],
       [ 1, -4, -4,  1]])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Valores de theta para las neuronas de la capa oculta</span>
<span class="c1">#</span>
<span class="n">nn</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([-1, -1, -2, -3])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># La red neuronal es equivalente a la siguiente función</span>
<span class="c1">#</span>
<span class="n">nn</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
step(
    + step(- 4x0 - 4x1 + x2 - 1)
    + step(x0 - 4x1 - 4x2 - 1)
    + step(x0 + x1 - 4x2 - 2)
    + step(x0 + x1 + x2 - 3)
    - 1
)
</pre></div></div>
</div>
</div>
<div class="section" id="Notas">
<h2>Notas<a class="headerlink" href="#Notas" title="Enlazar permanentemente con este título"></a></h2>
<p>El método de construcción del modelo no resulta adecuado para:</p>
<ul class="simple">
<li><p>Patrones de muchos bits (muchas entradas).</p></li>
<li><p>Más de una salida. En este caso se construye una red para salida.</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>