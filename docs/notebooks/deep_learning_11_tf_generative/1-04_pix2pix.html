<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2019 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Copyright 2019 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/deep_learning_11_tf_generative/1-04_pix2pix.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2019-The-TensorFlow-Authors.">
<h1>Copyright 2019 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2019-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<p>Licensed under the Apache License, Version 2.0 (the “License”);</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="pix2pix:-Image-to-image-translation-with-a-conditional-GAN">
<h2>pix2pix: Image-to-image translation with a conditional GAN<a class="headerlink" href="#pix2pix:-Image-to-image-translation-with-a-conditional-GAN" title="Enlazar permanentemente con este título"></a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>107be22be9db47d5a0acfa1f4f947a12|View on TensorFlow.org</p>
</td><td><p><a href="#id3"><span class="problematic" id="id4">|</span></a>a3e7a0b12c8c41a68a372a6cde3e4100|Run in Google Colab</p>
</td><td><p><a href="#id5"><span class="problematic" id="id6">|</span></a>9238bde8d3db4136a67a6013509af35f|View source on GitHub</p>
</td><td><p><a href="#id7"><span class="problematic" id="id8">|</span></a>363dce97342445c2a457b7ae3bdd873c|Download notebook</p>
</td></table><p>This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in <a class="reference external" href="https://arxiv.org/abs/1611.07004">Image-to-image translation with conditional adversarial networks</a> by Isola et al. (2017). pix2pix is not application specific—it can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white
images, turning Google Maps photos into aerial images, and even transforming sketches into photos.</p>
<p>In this example, your network will generate images of building facades using the <a class="reference external" href="http://cmp.felk.cvut.cz/~tylecr1/facade/">CMP Facade Database</a> provided by the <a class="reference external" href="http://cmp.felk.cvut.cz/">Center for Machine Perception</a> at the <a class="reference external" href="https://www.cvut.cz/">Czech Technical University in Prague</a>. To keep it short, you will use a <a class="reference external" href="(https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)">preprocessed copy</a> of this dataset created by the pix2pix authors.</p>
<p>In the pix2pix cGAN, you condition on input images and generate corresponding output images. cGANs were first proposed in <a class="reference external" href="https://arxiv.org/abs/1411.1784">Conditional Generative Adversarial Nets</a> (Mirza and Osindero, 2014)</p>
<p>The architecture of your network will contain:</p>
<ul class="simple">
<li><p>A generator with a <a class="reference external" href="%5BU-Net%5D(https://arxiv.org/abs/1505.04597)">U-Net</a>-based architecture.</p></li>
<li><p>A discriminator represented by a convolutional PatchGAN classifier (proposed in the <a class="reference external" href="https://arxiv.org/abs/1611.07004">pix2pix paper</a>).</p></li>
</ul>
<p>Note that each epoch can take around 15 seconds on a single V100 GPU.</p>
<p>Below are some examples of the output generated by the pix2pix cGAN after training for 200 epochs on the facades dataset (80k steps).</p>
<p><img alt="sample output_1" src="https://www.tensorflow.org/images/gan/pix2pix_1.png" /> <img alt="sample output_2" src="https://www.tensorflow.org/images/gan/pix2pix_2.png" /></p>
<div class="section" id="Import-TensorFlow-and-other-libraries">
<h3>Import TensorFlow and other libraries<a class="headerlink" href="#Import-TensorFlow-and-other-libraries" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow as tf

import os
import pathlib
import time
import datetime

from matplotlib import pyplot as plt
from IPython import display
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-the-dataset">
<h3>Load the dataset<a class="headerlink" href="#Load-the-dataset" title="Enlazar permanentemente con este título"></a></h3>
<p>Download the CMP Facade Database data (30MB). Additional datasets are available in the same format <a class="reference external" href="http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/">here</a>. In Colab you can select other datasets from the drop-down menu. Note that some of the other datasets are significantly larger (<code class="docutils literal notranslate"><span class="pre">edges2handbags</span></code> is 8GB).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>dataset_name = &quot;facades&quot; #@param [&quot;cityscapes&quot;, &quot;edges2handbags&quot;, &quot;edges2shoes&quot;, &quot;facades&quot;, &quot;maps&quot;, &quot;night2day&quot;]

</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>_URL = f&#39;http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz&#39;

path_to_zip = tf.keras.utils.get_file(
    fname=f&quot;{dataset_name}.tar.gz&quot;,
    origin=_URL,
    extract=True)

path_to_zip  = pathlib.Path(path_to_zip)

PATH = path_to_zip.parent/dataset_name
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>list(PATH.parent.iterdir())
</pre></div>
</div>
</div>
<p>Each original image is of size <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">x</span> <span class="pre">512</span></code> containing two <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">x</span> <span class="pre">256</span></code> images:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sample_image = tf.io.read_file(str(PATH / &#39;train/1.jpg&#39;))
sample_image = tf.io.decode_jpeg(sample_image)
print(sample_image.shape)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.figure()
plt.imshow(sample_image)
</pre></div>
</div>
</div>
<p>You need to separate real building facade images from the architecture label images—all of which will be of size <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">x</span> <span class="pre">256</span></code>.</p>
<p>Define a function that loads image files and outputs two image tensors:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def load(image_file):
  # Read and decode an image file to a uint8 tensor
  image = tf.io.read_file(image_file)
  image = tf.io.decode_jpeg(image)

  # Split each image tensor into two tensors:
  # - one with a real building facade image
  # - one with an architecture label image
  w = tf.shape(image)[1]
  w = w // 2
  input_image = image[:, w:, :]
  real_image = image[:, :w, :]

  # Convert both images to float32 tensors
  input_image = tf.cast(input_image, tf.float32)
  real_image = tf.cast(real_image, tf.float32)

  return input_image, real_image
</pre></div>
</div>
</div>
<p>Plot a sample of the input (architecture label image) and real (building facade photo) images:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>inp, re = load(str(PATH / &#39;train/100.jpg&#39;))
# Casting to int for matplotlib to display the images
plt.figure()
plt.imshow(inp / 255.0)
plt.figure()
plt.imshow(re / 255.0)
</pre></div>
</div>
</div>
<p>As described in the <a class="reference external" href="https://arxiv.org/abs/1611.07004">pix2pix paper</a>, you need to apply random jittering and mirroring to preprocess the training set.</p>
<p>Define several functions that:</p>
<ol class="arabic simple">
<li><p>Resize each <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">x</span> <span class="pre">256</span></code> image to a larger height and width—<code class="docutils literal notranslate"><span class="pre">286</span> <span class="pre">x</span> <span class="pre">286</span></code>.</p></li>
<li><p>Randomly crop it back to <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">x</span> <span class="pre">256</span></code>.</p></li>
<li><p>Randomly flip the image horizontally i.e. left to right (random mirroring).</p></li>
<li><p>Normalize the images to the <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code> range.</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># The facade training set consist of 400 images
BUFFER_SIZE = 400
# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment
BATCH_SIZE = 1
# Each image is 256x256 in size
IMG_WIDTH = 256
IMG_HEIGHT = 256
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def resize(input_image, real_image, height, width):
  input_image = tf.image.resize(input_image, [height, width],
                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  real_image = tf.image.resize(real_image, [height, width],
                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

  return input_image, real_image
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def random_crop(input_image, real_image):
  stacked_image = tf.stack([input_image, real_image], axis=0)
  cropped_image = tf.image.random_crop(
      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])

  return cropped_image[0], cropped_image[1]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Normalizing the images to [-1, 1]
def normalize(input_image, real_image):
  input_image = (input_image / 127.5) - 1
  real_image = (real_image / 127.5) - 1

  return input_image, real_image
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function()
def random_jitter(input_image, real_image):
  # Resizing to 286x286
  input_image, real_image = resize(input_image, real_image, 286, 286)

  # Random cropping back to 256x256
  input_image, real_image = random_crop(input_image, real_image)

  if tf.random.uniform(()) &gt; 0.5:
    # Random mirroring
    input_image = tf.image.flip_left_right(input_image)
    real_image = tf.image.flip_left_right(real_image)

  return input_image, real_image
</pre></div>
</div>
</div>
<p>You can inspect some of the preprocessed output:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.figure(figsize=(6, 6))
for i in range(4):
  rj_inp, rj_re = random_jitter(inp, re)
  plt.subplot(2, 2, i + 1)
  plt.imshow(rj_inp / 255.0)
  plt.axis(&#39;off&#39;)
plt.show()
</pre></div>
</div>
</div>
<p>Having checked that the loading and preprocessing works, let’s define a couple of helper functions that load and preprocess the training and test sets:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def load_image_train(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = random_jitter(input_image, real_image)
  input_image, real_image = normalize(input_image, real_image)

  return input_image, real_image
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def load_image_test(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = resize(input_image, real_image,
                                   IMG_HEIGHT, IMG_WIDTH)
  input_image, real_image = normalize(input_image, real_image)

  return input_image, real_image
</pre></div>
</div>
</div>
</div>
<div class="section" id="Build-an-input-pipeline-with-tf.data">
<h3>Build an input pipeline with <code class="docutils literal notranslate"><span class="pre">tf.data</span></code><a class="headerlink" href="#Build-an-input-pipeline-with-tf.data" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = tf.data.Dataset.list_files(str(PATH / &#39;train/*.jpg&#39;))
train_dataset = train_dataset.map(load_image_train,
                                  num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.batch(BATCH_SIZE)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>try:
  test_dataset = tf.data.Dataset.list_files(str(PATH / &#39;test/*.jpg&#39;))
except tf.errors.InvalidArgumentError:
  test_dataset = tf.data.Dataset.list_files(str(PATH / &#39;val/*.jpg&#39;))
test_dataset = test_dataset.map(load_image_test)
test_dataset = test_dataset.batch(BATCH_SIZE)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Build-the-generator">
<h3>Build the generator<a class="headerlink" href="#Build-the-generator" title="Enlazar permanentemente con este título"></a></h3>
<p>The generator of your pix2pix cGAN is a <em>modified</em> <a class="reference external" href="https://arxiv.org/abs/1505.04597">U-Net</a>. A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the <a class="reference external" href="https://www.tensorflow.org/tutorials/images/segmentation">Image segmentation</a> tutorial and on the <a class="reference external" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">U-Net project website</a>.)</p>
<ul class="simple">
<li><p>Each block in the encoder is: Convolution -&gt; Batch normalization -&gt; Leaky ReLU</p></li>
<li><p>Each block in the decoder is: Transposed convolution -&gt; Batch normalization -&gt; Dropout (applied to the first 3 blocks) -&gt; ReLU</p></li>
<li><p>There are skip connections between the encoder and decoder (as in the U-Net).</p></li>
</ul>
<p>Define the downsampler (encoder):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>OUTPUT_CHANNELS = 3
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def downsample(filters, size, apply_batchnorm=True):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
      tf.keras.layers.Conv2D(filters, size, strides=2, padding=&#39;same&#39;,
                             kernel_initializer=initializer, use_bias=False))

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

  result.add(tf.keras.layers.LeakyReLU())

  return result
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>down_model = downsample(3, 4)
down_result = down_model(tf.expand_dims(inp, 0))
print (down_result.shape)
</pre></div>
</div>
</div>
<p>Define the upsampler (decoder):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def upsample(filters, size, apply_dropout=False):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding=&#39;same&#39;,
                                    kernel_initializer=initializer,
                                    use_bias=False))

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
      result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return result
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>up_model = upsample(3, 4)
up_result = up_model(down_result)
print (up_result.shape)
</pre></div>
</div>
</div>
<p>Define the generator with the downsampler and the upsampler:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def Generator():
  inputs = tf.keras.layers.Input(shape=[256, 256, 3])

  down_stack = [
    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)
    downsample(128, 4),  # (batch_size, 64, 64, 128)
    downsample(256, 4),  # (batch_size, 32, 32, 256)
    downsample(512, 4),  # (batch_size, 16, 16, 512)
    downsample(512, 4),  # (batch_size, 8, 8, 512)
    downsample(512, 4),  # (batch_size, 4, 4, 512)
    downsample(512, 4),  # (batch_size, 2, 2, 512)
    downsample(512, 4),  # (batch_size, 1, 1, 512)
  ]

  up_stack = [
    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)
    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)
    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)
    upsample(512, 4),  # (batch_size, 16, 16, 1024)
    upsample(256, 4),  # (batch_size, 32, 32, 512)
    upsample(128, 4),  # (batch_size, 64, 64, 256)
    upsample(64, 4),  # (batch_size, 128, 128, 128)
  ]

  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding=&#39;same&#39;,
                                         kernel_initializer=initializer,
                                         activation=&#39;tanh&#39;)  # (batch_size, 256, 256, 3)

  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    x = tf.keras.layers.Concatenate()([x, skip])

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)
</pre></div>
</div>
</div>
<p>Visualize the generator model architecture:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>generator = Generator()
tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)
</pre></div>
</div>
</div>
<p>Test the generator:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>gen_output = generator(inp[tf.newaxis, ...], training=False)
plt.imshow(gen_output[0, ...])
</pre></div>
</div>
</div>
<div class="section" id="Define-the-generator-loss">
<h4>Define the generator loss<a class="headerlink" href="#Define-the-generator-loss" title="Enlazar permanentemente con este título"></a></h4>
<p>GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the <a class="reference external" href="https://arxiv.org/abs/1611.07004">pix2pix paper</a>.</p>
<ul class="simple">
<li><p>The generator loss is a sigmoid cross-entropy loss of the generated images and an <strong>array of ones</strong>.</p></li>
<li><p>The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.</p></li>
<li><p>This allows the generated image to become structurally similar to the target image.</p></li>
<li><p>The formula to calculate the total generator loss is <code class="docutils literal notranslate"><span class="pre">gan_loss</span> <span class="pre">+</span> <span class="pre">LAMBDA</span> <span class="pre">*</span> <span class="pre">l1_loss</span></code>, where <code class="docutils literal notranslate"><span class="pre">LAMBDA</span> <span class="pre">=</span> <span class="pre">100</span></code>. This value was decided by the authors of the paper.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LAMBDA = 100
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def generator_loss(disc_generated_output, gen_output, target):
  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)

  # Mean absolute error
  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))

  total_gen_loss = gan_loss + (LAMBDA * l1_loss)

  return total_gen_loss, gan_loss, l1_loss
</pre></div>
</div>
</div>
<p>The training procedure for the generator is as follows:</p>
<p><img alt="Generator Update Image" src="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1" /></p>
</div>
</div>
<div class="section" id="Build-the-discriminator">
<h3>Build the discriminator<a class="headerlink" href="#Build-the-discriminator" title="Enlazar permanentemente con este título"></a></h3>
<p>The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image <em>patch</em> is real or not real, as described in the <a class="reference external" href="https://arxiv.org/abs/1611.07004">pix2pix paper</a>.</p>
<ul class="simple">
<li><p>Each block in the discriminator is: Convolution -&gt; Batch normalization -&gt; Leaky ReLU.</p></li>
<li><p>The shape of the output after the last layer is <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">1)</span></code>.</p></li>
<li><p>Each <code class="docutils literal notranslate"><span class="pre">30</span> <span class="pre">x</span> <span class="pre">30</span></code> image patch of the output classifies a <code class="docutils literal notranslate"><span class="pre">70</span> <span class="pre">x</span> <span class="pre">70</span></code> portion of the input image.</p></li>
<li><p>The discriminator receives 2 inputs:</p>
<ul>
<li><p>The input image and the target image, which it should classify as real.</p></li>
<li><p>The input image and the generated image (the output of the generator), which it should classify as fake.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">tf.concat([inp,</span> <span class="pre">tar],</span> <span class="pre">axis=-1)</span></code> to concatenate these 2 inputs together.</p></li>
</ul>
</li>
</ul>
<p>Let’s define the discriminator:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def Discriminator():
  initializer = tf.random_normal_initializer(0., 0.02)

  inp = tf.keras.layers.Input(shape=[256, 256, 3], name=&#39;input_image&#39;)
  tar = tf.keras.layers.Input(shape=[256, 256, 3], name=&#39;target_image&#39;)

  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)

  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)
  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)
  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)

  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)
  conv = tf.keras.layers.Conv2D(512, 4, strides=1,
                                kernel_initializer=initializer,
                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)

  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)

  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)

  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)

  last = tf.keras.layers.Conv2D(1, 4, strides=1,
                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)

  return tf.keras.Model(inputs=[inp, tar], outputs=last)
</pre></div>
</div>
</div>
<p>Visualize the discriminator model architecture:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>discriminator = Discriminator()
tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)
</pre></div>
</div>
</div>
<p>Test the discriminator:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)
plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap=&#39;RdBu_r&#39;)
plt.colorbar()
</pre></div>
</div>
</div>
<div class="section" id="Define-the-discriminator-loss">
<h4>Define the discriminator loss<a class="headerlink" href="#Define-the-discriminator-loss" title="Enlazar permanentemente con este título"></a></h4>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">discriminator_loss</span></code> function takes 2 inputs: <strong>real images</strong> and <strong>generated images</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">real_loss</span></code> is a sigmoid cross-entropy loss of the <strong>real images</strong> and an <strong>array of ones(since these are the real images)</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">generated_loss</span></code> is a sigmoid cross-entropy loss of the <strong>generated images</strong> and an <strong>array of zeros (since these are the fake images)</strong>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">total_loss</span></code> is the sum of <code class="docutils literal notranslate"><span class="pre">real_loss</span></code> and <code class="docutils literal notranslate"><span class="pre">generated_loss</span></code>.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def discriminator_loss(disc_real_output, disc_generated_output):
  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)

  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)

  total_disc_loss = real_loss + generated_loss

  return total_disc_loss
</pre></div>
</div>
</div>
<p>The training procedure for the discriminator is shown below.</p>
<p>To learn more about the architecture and the hyperparameters you can refer to the <a class="reference external" href="https://arxiv.org/abs/1611.07004">pix2pix paper</a>.</p>
<p><img alt="Discriminator Update Image" src="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1" /></p>
</div>
</div>
<div class="section" id="Define-the-optimizers-and-a-checkpoint-saver">
<h3>Define the optimizers and a checkpoint-saver<a class="headerlink" href="#Define-the-optimizers-and-a-checkpoint-saver" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>checkpoint_dir = &#39;./training_checkpoints&#39;
checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt&quot;)
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-images">
<h3>Generate images<a class="headerlink" href="#Generate-images" title="Enlazar permanentemente con este título"></a></h3>
<p>Write a function to plot some images during training.</p>
<ul class="simple">
<li><p>Pass images from the test set to the generator.</p></li>
<li><p>The generator will then translate the input image into the output.</p></li>
<li><p>The last step is to plot the predictions and <em>voila</em>!</p></li>
</ul>
<p>Note: The <code class="docutils literal notranslate"><span class="pre">training=True</span></code> is intentional here since you want the batch statistics, while running the model on the test dataset. If you use <code class="docutils literal notranslate"><span class="pre">training=False</span></code>, you get the accumulated statistics learned from the training dataset (which you don’t want).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def generate_images(model, test_input, tar):
  prediction = model(test_input, training=True)
  plt.figure(figsize=(15, 15))

  display_list = [test_input[0], tar[0], prediction[0]]
  title = [&#39;Input Image&#39;, &#39;Ground Truth&#39;, &#39;Predicted Image&#39;]

  for i in range(3):
    plt.subplot(1, 3, i+1)
    plt.title(title[i])
    # Getting the pixel values in the [0, 1] range to plot.
    plt.imshow(display_list[i] * 0.5 + 0.5)
    plt.axis(&#39;off&#39;)
  plt.show()
</pre></div>
</div>
</div>
<p>Test the function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>for example_input, example_target in test_dataset.take(1):
  generate_images(generator, example_input, example_target)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h3>Training<a class="headerlink" href="#Training" title="Enlazar permanentemente con este título"></a></h3>
<ul class="simple">
<li><p>For each example input generates an output.</p></li>
<li><p>The discriminator receives the <code class="docutils literal notranslate"><span class="pre">input_image</span></code> and the generated image as the first input. The second input is the <code class="docutils literal notranslate"><span class="pre">input_image</span></code> and the <code class="docutils literal notranslate"><span class="pre">target_image</span></code>.</p></li>
<li><p>Next, calculate the generator and the discriminator loss.</p></li>
<li><p>Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.</p></li>
<li><p>Finally, log the losses to TensorBoard.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>log_dir=&quot;logs/&quot;

summary_writer = tf.summary.create_file_writer(
  log_dir + &quot;fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>@tf.function
def train_step(input_image, target, step):
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    gen_output = generator(input_image, training=True)

    disc_real_output = discriminator([input_image, target], training=True)
    disc_generated_output = discriminator([input_image, gen_output], training=True)

    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))

  with summary_writer.as_default():
    tf.summary.scalar(&#39;gen_total_loss&#39;, gen_total_loss, step=step//1000)
    tf.summary.scalar(&#39;gen_gan_loss&#39;, gen_gan_loss, step=step//1000)
    tf.summary.scalar(&#39;gen_l1_loss&#39;, gen_l1_loss, step=step//1000)
    tf.summary.scalar(&#39;disc_loss&#39;, disc_loss, step=step//1000)
</pre></div>
</div>
</div>
<p>The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.</p>
<ul class="simple">
<li><p>Iterates over the number of steps.</p></li>
<li><p>Every 10 steps print a dot (<code class="docutils literal notranslate"><span class="pre">.</span></code>).</p></li>
<li><p>Every 1k steps: clear the display and run <code class="docutils literal notranslate"><span class="pre">generate_images</span></code> to show the progress.</p></li>
<li><p>Every 5k steps: save a checkpoint.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def fit(train_ds, test_ds, steps):
  example_input, example_target = next(iter(test_ds.take(1)))
  start = time.time()

  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():
    if (step) % 1000 == 0:
      display.clear_output(wait=True)

      if step != 0:
        print(f&#39;Time taken for 1000 steps: {time.time()-start:.2f} sec\n&#39;)

      start = time.time()

      generate_images(generator, example_input, example_target)
      print(f&quot;Step: {step//1000}k&quot;)

    train_step(input_image, target, step)

    # Training step
    if (step+1) % 10 == 0:
      print(&#39;.&#39;, end=&#39;&#39;, flush=True)


    # Save (checkpoint) the model every 5k steps
    if (step + 1) % 5000 == 0:
      checkpoint.save(file_prefix=checkpoint_prefix)
</pre></div>
</div>
</div>
<p>This training loop saves logs that you can view in TensorBoard to monitor the training progress.</p>
<p>If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.</p>
<p>To launch the viewer paste the following into a code-cell:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%load_ext tensorboard
%tensorboard --logdir {log_dir}
</pre></div>
</div>
</div>
<p>Finally, run the training loop:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>fit(train_dataset, test_dataset, steps=40000)
</pre></div>
</div>
</div>
<p>If you want to share the TensorBoard results <em>publicly</em>, you can upload the logs to <a class="reference external" href="https://tensorboard.dev/">TensorBoard.dev</a> by copying the following into a code-cell.</p>
<p>Note: This requires a Google account.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>!tensorboard dev upload --logdir {log_dir}
</pre></div>
</div>
<p>Caution: This command does not terminate. It’s designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the “interrupt execution” option in your notebook tool.</p>
<p>You can view the <a class="reference external" href="https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw">results of a previous run</a> of this notebook on <a class="reference external" href="https://tensorboard.dev/">TensorBoard.dev</a>.</p>
<p>TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.</p>
<p>It can also included inline using an <code class="docutils literal notranslate"><span class="pre">&lt;iframe&gt;</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>display.IFrame(
    src=&quot;https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw&quot;,
    width=&quot;100%&quot;,
    height=&quot;1000px&quot;)
</pre></div>
</div>
</div>
<p>Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:</p>
<ul class="simple">
<li><p>Check that neither the generator nor the discriminator model has “won”. If either the <code class="docutils literal notranslate"><span class="pre">gen_gan_loss</span></code> or the <code class="docutils literal notranslate"><span class="pre">disc_loss</span></code> gets very low, it’s an indicator that this model is dominating the other, and you are not successfully training the combined model.</p></li>
<li><p>The value <code class="docutils literal notranslate"><span class="pre">log(2)</span> <span class="pre">=</span> <span class="pre">0.69</span></code> is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">disc_loss</span></code>, a value below <code class="docutils literal notranslate"><span class="pre">0.69</span></code> means the discriminator is doing better than random on the combined set of real and generated images.</p></li>
<li><p>For the <code class="docutils literal notranslate"><span class="pre">gen_gan_loss</span></code>, a value below <code class="docutils literal notranslate"><span class="pre">0.69</span></code> means the generator is doing better than random at fooling the discriminator.</p></li>
<li><p>As training progresses, the <code class="docutils literal notranslate"><span class="pre">gen_l1_loss</span></code> should go down.</p></li>
</ul>
</div>
<div class="section" id="Restore-the-latest-checkpoint-and-test-the-network">
<h3>Restore the latest checkpoint and test the network<a class="headerlink" href="#Restore-the-latest-checkpoint-and-test-the-network" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!ls {checkpoint_dir}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Restoring the latest checkpoint in checkpoint_dir
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-some-images-using-the-test-set">
<h3>Generate some images using the test set<a class="headerlink" href="#Generate-some-images-using-the-test-set" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Run the trained model on a few examples from the test set
for inp, tar in test_dataset.take(5):
  generate_images(generator, inp, tar)
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>