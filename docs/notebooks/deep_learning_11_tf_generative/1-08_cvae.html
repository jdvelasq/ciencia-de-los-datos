<!DOCTYPE html>
<html class="writer-html5" lang="es" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Copyright 2020 The TensorFlow Authors. &mdash; documentación de Cursos de Analítica y Machine Learning - </title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Cursos de Analítica y Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Buscar documentos" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Configuración</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../setup.html">Instalación de Vagrant y Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Pregrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../redes_neuronales_y_algoritmos_bioinspirados/index.html">Redes Neuronales Artificiales y Aprendizaje Profundo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fundamentos_de_analitica/index.html">Fundamentos de Analítica</a></li>
</ul>
<p class="caption"><span class="caption-text">Cursos de Posgrado</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_de_grandes_datos/index.html">Analítica de Grandes Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_predictiva/index.html">Analítica Predictiva</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ciencia_de_los_datos/index.html">Ciencia de los Datos Aplicada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../productos_de_datos/index.html">Productos de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analitica_avanzada/index.html">Analítica Avanzada</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cursos de Analítica y Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Copyright 2020 The TensorFlow Authors.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/notebooks/deep_learning_11_tf_generative/1-08_cvae.ipynb.txt" rel="nofollow"> Ver código fuente de la página</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Copyright-2020-The-TensorFlow-Authors.">
<h1>Copyright 2020 The TensorFlow Authors.<a class="headerlink" href="#Copyright-2020-The-TensorFlow-Authors." title="Enlazar permanentemente con este título"></a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>#@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
</pre></div>
</div>
</div>
<div class="section" id="Convolutional-Variational-Autoencoder">
<h2>Convolutional Variational Autoencoder<a class="headerlink" href="#Convolutional-Variational-Autoencoder" title="Enlazar permanentemente con este título"></a></h2>
<table class="tfo-notebook-buttons" align="left"><td><p><img alt="4d52e400f06e404b992dbead38e00919" src="https://www.tensorflow.org/images/tf_logo_32px.png" /> View on TensorFlow.org</p>
</td><td><p><img alt="8482521c1bc24604a6cebe785c3319ce" src="https://www.tensorflow.org/images/colab_logo_32px.png" /> Run in Google Colab</p>
</td><td><p><img alt="325c9ed72b36442984eef330fb20316f" src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" /> View source on GitHub</p>
</td><td><p><a href="#id1"><span class="problematic" id="id2">|</span></a>6dd4760f89404e6489d0a80c304934cc|Download notebook</p>
</td></table><p>This notebook demonstrates how to train a Variational Autoencoder (VAE) (<a class="reference external" href="https://arxiv.org/abs/1312.6114">1</a>, <a class="reference external" href="https://arxiv.org/abs/1401.4082">2</a>) on the MNIST dataset. A VAE is a probabilistic take on the autoencoder, a model which takes high dimensional input data and compresses it into a smaller representation. Unlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and
variance of a Gaussian. This approach produces a continuous, structured latent space, which is useful for image generation.</p>
<p><img alt="CVAE image latent space" src="../../_images/cvae_latent_space.jpg" /></p>
<div class="section" id="Setup">
<h3>Setup<a class="headerlink" href="#Setup" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!pip install tensorflow-probability

# to generate gifs
!pip install imageio
!pip install git+https://github.com/tensorflow/docs
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from IPython import display

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
import tensorflow_probability as tfp
import time
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-the-MNIST-dataset">
<h3>Load the MNIST dataset<a class="headerlink" href="#Load-the-MNIST-dataset" title="Enlazar permanentemente con este título"></a></h3>
<p>Each MNIST image is originally a vector of 784 integers, each of which is between 0-255 and represents the intensity of a pixel. Model each pixel with a Bernoulli distribution in our model, and statically binarize the dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def preprocess_images(images):
  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.
  return np.where(images &gt; .5, 1.0, 0.0).astype(&#39;float32&#39;)

train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_size = 60000
batch_size = 32
test_size = 10000
</pre></div>
</div>
</div>
</div>
<div class="section" id="Use-tf.data-to-batch-and-shuffle-the-data">
<h3>Use <em>tf.data</em> to batch and shuffle the data<a class="headerlink" href="#Use-tf.data-to-batch-and-shuffle-the-data" title="Enlazar permanentemente con este título"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)
                 .shuffle(train_size).batch(batch_size))
test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)
                .shuffle(test_size).batch(batch_size))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Define-the-encoder-and-decoder-networks-with-tf.keras.Sequential">
<h3>Define the encoder and decoder networks with <em>tf.keras.Sequential</em><a class="headerlink" href="#Define-the-encoder-and-decoder-networks-with-tf.keras.Sequential" title="Enlazar permanentemente con este título"></a></h3>
<p>In this VAE example, use two small ConvNets for the encoder and decoder networks. In the literature, these networks are also referred to as inference/recognition and generative models respectively. Use <code class="docutils literal notranslate"><span class="pre">tf.keras.Sequential</span></code> to simplify implementation. Let <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(z\)</span> denote the observation and latent variable respectively in the following descriptions.</p>
<div class="section" id="Encoder-network">
<h4>Encoder network<a class="headerlink" href="#Encoder-network" title="Enlazar permanentemente con este título"></a></h4>
<p>This defines the approximate posterior distribution <span class="math notranslate nohighlight">\(q(z|x)\)</span>, which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation <span class="math notranslate nohighlight">\(z\)</span>. In this example, simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance parameters of a factorized Gaussian. Output log-variance instead of the variance directly for numerical stability.</p>
</div>
<div class="section" id="Decoder-network">
<h4>Decoder network<a class="headerlink" href="#Decoder-network" title="Enlazar permanentemente con este título"></a></h4>
<p>This defines the conditional distribution of the observation <span class="math notranslate nohighlight">\(p(x|z)\)</span>, which takes a latent sample <span class="math notranslate nohighlight">\(z\)</span> as input and outputs the parameters for a conditional distribution of the observation. Model the latent distribution prior <span class="math notranslate nohighlight">\(p(z)\)</span> as a unit Gaussian.</p>
</div>
<div class="section" id="Reparameterization-trick">
<h4>Reparameterization trick<a class="headerlink" href="#Reparameterization-trick" title="Enlazar permanentemente con este título"></a></h4>
<p>To generate a sample <span class="math notranslate nohighlight">\(z\)</span> for the decoder during training, you can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation <span class="math notranslate nohighlight">\(x\)</span>. However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.</p>
<p>To address this, use a reparameterization trick. In our example, you approximate <span class="math notranslate nohighlight">\(z\)</span> using the decoder parameters and another parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[z = \mu + \sigma \odot \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the decoder output. The <span class="math notranslate nohighlight">\(\epsilon\)</span> can be thought of as a random noise used to maintain stochasticity of <span class="math notranslate nohighlight">\(z\)</span>. Generate <span class="math notranslate nohighlight">\(\epsilon\)</span> from a standard normal distribution.</p>
<p>The latent variable <span class="math notranslate nohighlight">\(z\)</span> is now generated by a function of <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span>, which would enable the model to backpropagate gradients in the encoder through <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> respectively, while maintaining stochasticity through <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
</div>
<div class="section" id="Network-architecture">
<h4>Network architecture<a class="headerlink" href="#Network-architecture" title="Enlazar permanentemente con este título"></a></h4>
<p>For the encoder network, use two convolutional layers followed by a fully-connected layer. In the decoder network, mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it’s common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>class CVAE(tf.keras.Model):
  &quot;&quot;&quot;Convolutional variational autoencoder.&quot;&quot;&quot;

  def __init__(self, latent_dim):
    super(CVAE, self).__init__()
    self.latent_dim = latent_dim
    self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(
                filters=32, kernel_size=3, strides=(2, 2), activation=&#39;relu&#39;),
            tf.keras.layers.Conv2D(
                filters=64, kernel_size=3, strides=(2, 2), activation=&#39;relu&#39;),
            tf.keras.layers.Flatten(),
            # No activation
            tf.keras.layers.Dense(latent_dim + latent_dim),
        ]
    )

    self.decoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),
            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),
            tf.keras.layers.Conv2DTranspose(
                filters=64, kernel_size=3, strides=2, padding=&#39;same&#39;,
                activation=&#39;relu&#39;),
            tf.keras.layers.Conv2DTranspose(
                filters=32, kernel_size=3, strides=2, padding=&#39;same&#39;,
                activation=&#39;relu&#39;),
            # No activation
            tf.keras.layers.Conv2DTranspose(
                filters=1, kernel_size=3, strides=1, padding=&#39;same&#39;),
        ]
    )

  @tf.function
  def sample(self, eps=None):
    if eps is None:
      eps = tf.random.normal(shape=(100, self.latent_dim))
    return self.decode(eps, apply_sigmoid=True)

  def encode(self, x):
    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return mean, logvar

  def reparameterize(self, mean, logvar):
    eps = tf.random.normal(shape=mean.shape)
    return eps * tf.exp(logvar * .5) + mean

  def decode(self, z, apply_sigmoid=False):
    logits = self.decoder(z)
    if apply_sigmoid:
      probs = tf.sigmoid(logits)
      return probs
    return logits
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Define-the-loss-function-and-the-optimizer">
<h3>Define the loss function and the optimizer<a class="headerlink" href="#Define-the-loss-function-and-the-optimizer" title="Enlazar permanentemente con este título"></a></h3>
<p>VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:</p>
<div class="math notranslate nohighlight">
\[\log p(x) \ge \text{ELBO} = \mathbb{E}_{q(z|x)}\left[\log \frac{p(x, z)}{q(z|x)}\right].\]</div>
<p>In practice, optimize the single sample Monte Carlo estimate of this expectation:</p>
<div class="math notranslate nohighlight">
\[\log p(x| z) + \log p(z) - \log q(z|x),\]</div>
<p>where <span class="math notranslate nohighlight">\(z\)</span> is sampled from <span class="math notranslate nohighlight">\(q(z|x)\)</span>.</p>
<p>Note: You could also analytically compute the KL term, but here you incorporate all three terms in the Monte Carlo estimator for simplicity.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>optimizer = tf.keras.optimizers.Adam(1e-4)


def log_normal_pdf(sample, mean, logvar, raxis=1):
  log2pi = tf.math.log(2. * np.pi)
  return tf.reduce_sum(
      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),
      axis=raxis)


def compute_loss(model, x):
  mean, logvar = model.encode(x)
  z = model.reparameterize(mean, logvar)
  x_logit = model.decode(z)
  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])
  logpz = log_normal_pdf(z, 0., 0.)
  logqz_x = log_normal_pdf(z, mean, logvar)
  return -tf.reduce_mean(logpx_z + logpz - logqz_x)


@tf.function
def train_step(model, x, optimizer):
  &quot;&quot;&quot;Executes one training step and returns the loss.

  This function computes the loss and gradients, and uses the latter to
  update the model&#39;s parameters.
  &quot;&quot;&quot;
  with tf.GradientTape() as tape:
    loss = compute_loss(model, x)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</pre></div>
</div>
</div>
</div>
<div class="section" id="Training">
<h3>Training<a class="headerlink" href="#Training" title="Enlazar permanentemente con este título"></a></h3>
<ul class="simple">
<li><p>Start by iterating over the dataset</p></li>
<li><p>During each iteration, pass the image to the encoder to obtain a set of mean and log-variance parameters of the approximate posterior <span class="math notranslate nohighlight">\(q(z|x)\)</span></p></li>
<li><p>then apply the <em>reparameterization trick</em> to sample from <span class="math notranslate nohighlight">\(q(z|x)\)</span></p></li>
<li><p>Finally, pass the reparameterized samples to the decoder to obtain the logits of the generative distribution <span class="math notranslate nohighlight">\(p(x|z)\)</span></p></li>
<li><p>Note: Since you use the dataset loaded by keras with 60k datapoints in the training set and 10k datapoints in the test set, our resulting ELBO on the test set is slightly higher than reported results in the literature which uses dynamic binarization of Larochelle’s MNIST.</p></li>
</ul>
<div class="section" id="Generating-images">
<h4>Generating images<a class="headerlink" href="#Generating-images" title="Enlazar permanentemente con este título"></a></h4>
<ul class="simple">
<li><p>After training, it is time to generate some images</p></li>
<li><p>Start by sampling a set of latent vectors from the unit Gaussian prior distribution <span class="math notranslate nohighlight">\(p(z)\)</span></p></li>
<li><p>The generator will then convert the latent sample <span class="math notranslate nohighlight">\(z\)</span> to logits of the observation, giving a distribution <span class="math notranslate nohighlight">\(p(x|z)\)</span></p></li>
<li><p>Here, plot the probabilities of Bernoulli distributions</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>epochs = 10
# set the dimensionality of the latent space to a plane for visualization later
latent_dim = 2
num_examples_to_generate = 16

# keeping the random vector constant for generation (prediction) so
# it will be easier to see the improvement.
random_vector_for_generation = tf.random.normal(
    shape=[num_examples_to_generate, latent_dim])
model = CVAE(latent_dim)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def generate_and_save_images(model, epoch, test_sample):
  mean, logvar = model.encode(test_sample)
  z = model.reparameterize(mean, logvar)
  predictions = model.sample(z)
  fig = plt.figure(figsize=(4, 4))

  for i in range(predictions.shape[0]):
    plt.subplot(4, 4, i + 1)
    plt.imshow(predictions[i, :, :, 0], cmap=&#39;gray&#39;)
    plt.axis(&#39;off&#39;)

  # tight_layout minimizes the overlap between 2 sub-plots
  plt.savefig(&#39;image_at_epoch_{:04d}.png&#39;.format(epoch))
  plt.show()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># Pick a sample of the test set for generating output images
assert batch_size &gt;= num_examples_to_generate
for test_batch in test_dataset.take(1):
  test_sample = test_batch[0:num_examples_to_generate, :, :, :]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>generate_and_save_images(model, 0, test_sample)

for epoch in range(1, epochs + 1):
  start_time = time.time()
  for train_x in train_dataset:
    train_step(model, train_x, optimizer)
  end_time = time.time()

  loss = tf.keras.metrics.Mean()
  for test_x in test_dataset:
    loss(compute_loss(model, test_x))
  elbo = -loss.result()
  display.clear_output(wait=False)
  print(&#39;Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}&#39;
        .format(epoch, elbo, end_time - start_time))
  generate_and_save_images(model, epoch, test_sample)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Display-a-generated-image-from-the-last-training-epoch">
<h4>Display a generated image from the last training epoch<a class="headerlink" href="#Display-a-generated-image-from-the-last-training-epoch" title="Enlazar permanentemente con este título"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def display_image(epoch_no):
  return PIL.Image.open(&#39;image_at_epoch_{:04d}.png&#39;.format(epoch_no))
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plt.imshow(display_image(epoch))
plt.axis(&#39;off&#39;)  # Display images
</pre></div>
</div>
</div>
</div>
<div class="section" id="Display-an-animated-GIF-of-all-the-saved-images">
<h4>Display an animated GIF of all the saved images<a class="headerlink" href="#Display-an-animated-GIF-of-all-the-saved-images" title="Enlazar permanentemente con este título"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>anim_file = &#39;cvae.gif&#39;

with imageio.get_writer(anim_file, mode=&#39;I&#39;) as writer:
  filenames = glob.glob(&#39;image*.png&#39;)
  filenames = sorted(filenames)
  for filename in filenames:
    image = imageio.imread(filename)
    writer.append_data(image)
  image = imageio.imread(filename)
  writer.append_data(image)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import tensorflow_docs.vis.embed as embed
embed.embed_file(anim_file)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Display-a-2D-manifold-of-digits-from-the-latent-space">
<h4>Display a 2D manifold of digits from the latent space<a class="headerlink" href="#Display-a-2D-manifold-of-digits-from-the-latent-space" title="Enlazar permanentemente con este título"></a></h4>
<p>Running the code below will show a continuous distribution of the different digit classes, with each digit morphing into another across the 2D latent space. Use <a class="reference external" href="https://www.tensorflow.org/probability">TensorFlow Probability</a> to generate a standard normal distribution for the latent space.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def plot_latent_images(model, n, digit_size=28):
  &quot;&quot;&quot;Plots n x n digit images decoded from the latent space.&quot;&quot;&quot;

  norm = tfp.distributions.Normal(0, 1)
  grid_x = norm.quantile(np.linspace(0.05, 0.95, n))
  grid_y = norm.quantile(np.linspace(0.05, 0.95, n))
  image_width = digit_size*n
  image_height = image_width
  image = np.zeros((image_height, image_width))

  for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
      z = np.array([[xi, yi]])
      x_decoded = model.sample(z)
      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))
      image[i * digit_size: (i + 1) * digit_size,
            j * digit_size: (j + 1) * digit_size] = digit.numpy()

  plt.figure(figsize=(10, 10))
  plt.imshow(image, cmap=&#39;Greys_r&#39;)
  plt.axis(&#39;Off&#39;)
  plt.show()
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>plot_latent_images(model, 20)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Next-steps">
<h3>Next steps<a class="headerlink" href="#Next-steps" title="Enlazar permanentemente con este título"></a></h3>
<p>This tutorial has demonstrated how to implement a convolutional variational autoencoder using TensorFlow.</p>
<p>As a next step, you could try to improve the model output by increasing the network size. For instance, you could try setting the <code class="docutils literal notranslate"><span class="pre">filter</span></code> parameters for each of the <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> and <code class="docutils literal notranslate"><span class="pre">Conv2DTranspose</span></code> layers to 512. Note that in order to generate the final 2D latent image plot, you would need to keep <code class="docutils literal notranslate"><span class="pre">latent_dim</span></code> to 2. Also, the training time would increase as the network size increases.</p>
<p>You could also try implementing a VAE using a different dataset, such as CIFAR-10.</p>
<p>VAEs can be implemented in several different styles and of varying complexity. You can find additional implementations in the following sources: - <a class="reference external" href="https://keras.io/examples/generative/vae/">Variational AutoEncoder (keras.io)</a> - <a class="reference external" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example">VAE example from “Writing custom layers and models” guide (tensorflow.org)</a> - <a class="reference external" href="https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE">TFP Probabilistic Layers: Variational Auto
Encoder</a></p>
<p>If you’d like to learn more about the details of VAEs, please refer to <a class="reference external" href="https://arxiv.org/abs/1906.02691">An Introduction to Variational Autoencoders</a>.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Derechos de autor 2019-2021, Juan D. Velasquez.</p>
  </div>

  Compilado con <a href="https://www.sphinx-doc.org/">Sphinx</a> usando un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">tema</a>
    proporcionado por <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXX-1', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>